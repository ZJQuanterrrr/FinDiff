{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bl6_81TE1Hqp",
        "outputId": "0971576f-9946-437c-b489-200422fae63f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Requirement already satisfied: dython in /usr/local/lib/python3.11/dist-packages (0.7.9)\n",
            "Requirement already satisfied: icecream in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: sdv==1.4.0 in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: category-encoders in /usr/local/lib/python3.11/dist-packages (2.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: boto3<2,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from sdv==1.4.0) (1.38.23)\n",
            "Requirement already satisfied: botocore<2,>=1.18 in /usr/local/lib/python3.11/dist-packages (from sdv==1.4.0) (1.38.23)\n",
            "Requirement already satisfied: cloudpickle<3.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from sdv==1.4.0) (2.2.1)\n",
            "Requirement already satisfied: Faker<15,>=10 in /usr/local/lib/python3.11/dist-packages (from sdv==1.4.0) (14.2.1)\n",
            "Requirement already satisfied: graphviz<1,>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from sdv==1.4.0) (0.20.3)\n",
            "Requirement already satisfied: tqdm<5,>=4.15 in /usr/local/lib/python3.11/dist-packages (from sdv==1.4.0) (4.67.1)\n",
            "Requirement already satisfied: copulas<0.10,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from sdv==1.4.0) (0.9.2)\n",
            "Requirement already satisfied: ctgan<0.8,>=0.7.4 in /usr/local/lib/python3.11/dist-packages (from sdv==1.4.0) (0.7.5)\n",
            "Requirement already satisfied: deepecho<0.5,>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sdv==1.4.0) (0.4.2)\n",
            "Requirement already satisfied: rdt<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from sdv==1.4.0) (1.7.0)\n",
            "Requirement already satisfied: sdmetrics<0.12,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from sdv==1.4.0) (0.11.1)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.23.3 in /usr/local/lib/python3.11/dist-packages (from sdv==1.4.0) (1.24.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.15.0->sdv==1.4.0) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.15.0->sdv==1.4.0) (0.13.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<2,>=1.18->sdv==1.4.0) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<2,>=1.18->sdv==1.4.0) (2.4.0)\n",
            "Requirement already satisfied: matplotlib<4,>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from copulas<0.10,>=0.9.0->sdv==1.4.0) (3.10.0)\n",
            "Requirement already satisfied: scipy<2,>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from copulas<0.10,>=0.9.0->sdv==1.4.0) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from ctgan<0.8,>=0.7.4->sdv==1.4.0) (1.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv==1.4.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv==1.4.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv==1.4.0) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv==1.4.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv==1.4.0) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv==1.4.0) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv==1.4.0) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<2,>=1.18->sdv==1.4.0) (1.17.0)\n",
            "Requirement already satisfied: psutil<6,>=5.7 in /usr/local/lib/python3.11/dist-packages (from rdt<2,>=1.7.0->sdv==1.4.0) (5.9.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1.1.3->ctgan<0.8,>=0.7.4->sdv==1.4.0) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1.1.3->ctgan<0.8,>=0.7.4->sdv==1.4.0) (3.6.0)\n",
            "Requirement already satisfied: plotly<6,>=5.10.0 in /usr/local/lib/python3.11/dist-packages (from sdmetrics<0.12,>=0.11.0->sdv==1.4.0) (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly<6,>=5.10.0->sdmetrics<0.12,>=0.11.0->sdv==1.4.0) (9.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from dython) (0.13.2)\n",
            "Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from icecream) (0.4.6)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from icecream) (2.19.1)\n",
            "Requirement already satisfied: executing>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from icecream) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from icecream) (3.0.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category-encoders) (1.0.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category-encoders) (0.14.4)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "fatal: destination path 'tab-ddpm' already exists and is not an empty directory.\n",
            "/content/tab-ddpm\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip\n",
        "!pip install torch==2.0.1 pandas openpyxl toml dython icecream optuna sdv==1.4.0 category-encoders\n",
        "\n",
        "!git clone https://github.com/yandex-research/tab-ddpm.git\n",
        "%cd tab-ddpm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tomli_w\n",
        "!pip install catboost\n",
        "!pip install skorch\n",
        "!pip install tomli\n",
        "!pip install zero\n",
        "!pip install rtdl\n",
        "!pip install sdv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1uvwjyK6BpO",
        "outputId": "be9b9d87-9950-4a38-c1b1-1a2ba44910c3",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tomli_w\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
            "Installing collected packages: tomli_w\n",
            "Successfully installed tomli_w-1.2.0\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.24.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n",
            "Collecting skorch\n",
            "  Downloading skorch-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from skorch) (1.24.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from skorch) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from skorch) (1.15.3)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.11/dist-packages (from skorch) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.0->skorch) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.0->skorch) (3.6.0)\n",
            "Downloading skorch-1.1.0-py3-none-any.whl (228 kB)\n",
            "Installing collected packages: skorch\n",
            "Successfully installed skorch-1.1.0\n",
            "Collecting tomli\n",
            "  Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "Installing collected packages: tomli\n",
            "Successfully installed tomli-2.2.1\n",
            "Collecting zero\n",
            "  Downloading zero-0.9.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy>=1.15.2 in /usr/local/lib/python3.11/dist-packages (from zero) (1.24.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from zero) (1.15.3)\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from zero) (3.10.0)\n",
            "Requirement already satisfied: requests>=2.19.1 in /usr/local/lib/python3.11/dist-packages (from zero) (2.32.3)\n",
            "Requirement already satisfied: progressbar2>=3.38.0 in /usr/local/lib/python3.11/dist-packages (from zero) (4.5.0)\n",
            "Requirement already satisfied: tabulate>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from zero) (0.9.0)\n",
            "Collecting setuptools-scm>=3.1.0 (from zero)\n",
            "  Downloading setuptools_scm-8.3.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: ply>=3.11 in /usr/local/lib/python3.11/dist-packages (from zero) (3.11)\n",
            "Collecting Click==7.0 (from zero)\n",
            "  Downloading Click-7.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting quantiphy>=2.5.0 (from zero)\n",
            "  Downloading quantiphy-2.20-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.11/dist-packages (from zero) (6.0.2)\n",
            "Requirement already satisfied: graphviz>=0.9 in /usr/local/lib/python3.11/dist-packages (from zero) (0.20.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->zero) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->zero) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->zero) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->zero) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->zero) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->zero) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->zero) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->zero) (2.9.0.post0)\n",
            "Requirement already satisfied: python-utils>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from progressbar2>=3.38.0->zero) (3.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.3->zero) (1.17.0)\n",
            "Requirement already satisfied: typing_extensions>3.10.0.2 in /usr/local/lib/python3.11/dist-packages (from python-utils>=3.8.1->progressbar2>=3.38.0->zero) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.1->zero) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.1->zero) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.1->zero) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.1->zero) (2025.4.26)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from setuptools-scm>=3.1.0->zero) (75.2.0)\n",
            "Downloading zero-0.9.2-py3-none-any.whl (258 kB)\n",
            "Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
            "Downloading quantiphy-2.20-py3-none-any.whl (41 kB)\n",
            "Downloading setuptools_scm-8.3.1-py3-none-any.whl (43 kB)\n",
            "Installing collected packages: setuptools-scm, quantiphy, Click, zero\n",
            "\u001b[2K  Attempting uninstall: Click\n",
            "\u001b[2K    Found existing installation: click 8.2.0\n",
            "\u001b[2K    Uninstalling click-8.2.0:\n",
            "\u001b[2K      Successfully uninstalled click-8.2.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [zero]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "typer 0.15.3 requires click>=8.0.0, but you have click 7.0 which is incompatible.\n",
            "distributed 2024.12.1 requires click>=8.0, but you have click 7.0 which is incompatible.\n",
            "distributed 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
            "wandb 0.19.11 requires click!=8.0.0,>=7.1, but you have click 7.0 which is incompatible.\n",
            "dask 2024.12.1 requires click>=8.1, but you have click 7.0 which is incompatible.\n",
            "dask 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
            "flask 3.1.1 requires click>=8.1.3, but you have click 7.0 which is incompatible.\n",
            "dask-cuda 25.2.0 requires click>=8.1, but you have click 7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Click-7.0 quantiphy-2.20 setuptools-scm-8.3.1 zero-0.9.2\n",
            "Collecting rtdl\n",
            "  Downloading rtdl-0.0.13-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.11/dist-packages (from rtdl) (1.24.4)\n",
            "Collecting torch<2,>=1.7 (from rtdl)\n",
            "  Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch<2,>=1.7->rtdl) (4.13.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch<2,>=1.7->rtdl) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch<2,>=1.7->rtdl) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch<2,>=1.7->rtdl) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch<2,>=1.7->rtdl) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.7->rtdl) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.7->rtdl) (0.45.1)\n",
            "Downloading rtdl-0.0.13-py3-none-any.whl (23 kB)\n",
            "Downloading torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch, rtdl\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 2.0.1\n",
            "\u001b[2K    Uninstalling torch-2.0.1:\n",
            "\u001b[2K      Successfully uninstalled torch-2.0.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [rtdl]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ctgan 0.7.5 requires torch>=2.0.0; python_version >= \"3.11\", but you have torch 1.13.1 which is incompatible.\n",
            "deepecho 0.4.2 requires torch>=2.0.0; python_version >= \"3.11\", but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\n",
            "accelerate 1.6.0 requires torch>=2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.11.0 requires torch>=2, but you have torch 1.13.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed rtdl-0.0.13 torch-1.13.1\n",
            "Requirement already satisfied: sdv in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: boto3<2,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (1.38.23)\n",
            "Requirement already satisfied: botocore<2,>=1.18 in /usr/local/lib/python3.11/dist-packages (from sdv) (1.38.23)\n",
            "Requirement already satisfied: cloudpickle<3.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (2.2.1)\n",
            "Requirement already satisfied: Faker<15,>=10 in /usr/local/lib/python3.11/dist-packages (from sdv) (14.2.1)\n",
            "Requirement already satisfied: graphviz<1,>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.20.3)\n",
            "Requirement already satisfied: tqdm<5,>=4.15 in /usr/local/lib/python3.11/dist-packages (from sdv) (4.67.1)\n",
            "Requirement already satisfied: copulas<0.10,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.9.2)\n",
            "Requirement already satisfied: ctgan<0.8,>=0.7.4 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.7.5)\n",
            "Requirement already satisfied: deepecho<0.5,>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.4.2)\n",
            "Requirement already satisfied: rdt<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (1.7.0)\n",
            "Requirement already satisfied: sdmetrics<0.12,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.11.1)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.23.3 in /usr/local/lib/python3.11/dist-packages (from sdv) (1.24.4)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (2.2.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.15.0->sdv) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.15.0->sdv) (0.13.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<2,>=1.18->sdv) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<2,>=1.18->sdv) (2.4.0)\n",
            "Requirement already satisfied: matplotlib<4,>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from copulas<0.10,>=0.9.0->sdv) (3.10.0)\n",
            "Requirement already satisfied: scipy<2,>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from copulas<0.10,>=0.9.0->sdv) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from ctgan<0.8,>=0.7.4->sdv) (1.6.1)\n",
            "Collecting torch>=2.0.0 (from ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.6.0->copulas<0.10,>=0.9.0->sdv) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<2,>=1.18->sdv) (1.17.0)\n",
            "Requirement already satisfied: psutil<6,>=5.7 in /usr/local/lib/python3.11/dist-packages (from rdt<2,>=1.7.0->sdv) (5.9.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1.1.3->ctgan<0.8,>=0.7.4->sdv) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1.1.3->ctgan<0.8,>=0.7.4->sdv) (3.6.0)\n",
            "Requirement already satisfied: plotly<6,>=5.10.0 in /usr/local/lib/python3.11/dist-packages (from sdmetrics<0.12,>=0.11.0->sdv) (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly<6,>=5.10.0->sdmetrics<0.12,>=0.11.0->sdv) (9.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->sdv) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->sdv) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv) (4.13.2)\n",
            "Collecting sympy>=1.13.3 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.0 (from torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv)\n",
            "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->ctgan<0.8,>=0.7.4->sdv) (3.0.2)\n",
            "Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m181.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m208.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m132.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m134.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m165.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m146.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m155.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "\u001b[2K  Attempting uninstall: triton\n",
            "\u001b[2K    Found existing installation: triton 2.0.0\n",
            "\u001b[2K    Uninstalling triton-2.0.0:\n",
            "\u001b[2K      Successfully uninstalled triton-2.0.0\n",
            "\u001b[2K  Attempting uninstall: sympy\n",
            "\u001b[2K    Found existing installation: sympy 1.13.1\n",
            "\u001b[2K    Uninstalling sympy-1.13.1:\n",
            "\u001b[2K      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "\u001b[2K  Attempting uninstall: nvidia-curand-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 1.13.1\n",
            "\u001b[2K    Uninstalling torch-1.13.1:\n",
            "\u001b[2K      Successfully uninstalled torch-1.13.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [torch]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "rtdl 0.0.13 requires torch<2,>=1.7, but you have torch 2.7.0 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.0 triton-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/train.py\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import zero\n",
        "\n",
        "import sys\n",
        "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n",
        "\n",
        "from tab_ddpm import GaussianMultinomialDiffusion\n",
        "from utils_train import get_model, make_dataset, update_ema\n",
        "import lib\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, diffusion, train_iter, lr, weight_decay, steps, device=torch.device('cuda:1')):\n",
        "        self.diffusion = diffusion\n",
        "        self.ema_model = deepcopy(self.diffusion._denoise_fn)\n",
        "        for param in self.ema_model.parameters():\n",
        "            param.detach_()\n",
        "\n",
        "        self.train_iter = train_iter\n",
        "        self.steps = steps\n",
        "        self.init_lr = lr\n",
        "        self.optimizer = torch.optim.AdamW(self.diffusion.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        self.device = device\n",
        "        self.loss_history = pd.DataFrame(columns=['step', 'mloss', 'gloss', 'loss'])\n",
        "        self.log_every = 100\n",
        "        self.print_every = 500\n",
        "        self.ema_every = 1000\n",
        "\n",
        "    def _anneal_lr(self, step):\n",
        "        frac_done = step / self.steps\n",
        "        lr = self.init_lr * (1 - frac_done)\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "\n",
        "    def _run_step(self, x, out_dict):\n",
        "        x = x.to(self.device)\n",
        "        for k in out_dict:\n",
        "            out_dict[k] = out_dict[k].long().to(self.device)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss_multi, loss_gauss = self.diffusion.mixed_loss(x, out_dict)\n",
        "        loss = loss_multi + loss_gauss\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss_multi, loss_gauss\n",
        "\n",
        "    def run_loop(self):\n",
        "        step = 0\n",
        "        curr_loss_multi = 0.0\n",
        "        curr_loss_gauss = 0.0\n",
        "\n",
        "        curr_count = 0\n",
        "        while step < self.steps:\n",
        "            x, out_dict = next(self.train_iter)\n",
        "            out_dict = {'y': out_dict}\n",
        "            batch_loss_multi, batch_loss_gauss = self._run_step(x, out_dict)\n",
        "\n",
        "            self._anneal_lr(step)\n",
        "\n",
        "            curr_count += len(x)\n",
        "            curr_loss_multi += batch_loss_multi.item() * len(x)\n",
        "            curr_loss_gauss += batch_loss_gauss.item() * len(x)\n",
        "\n",
        "            if (step + 1) % self.log_every == 0:\n",
        "                mloss = np.around(curr_loss_multi / curr_count, 4)\n",
        "                gloss = np.around(curr_loss_gauss / curr_count, 4)\n",
        "                if (step + 1) % self.print_every == 0:\n",
        "                    print(f'Step {(step + 1)}/{self.steps} MLoss: {mloss} GLoss: {gloss} Sum: {mloss + gloss}')\n",
        "                self.loss_history.loc[len(self.loss_history)] =[step + 1, mloss, gloss, mloss + gloss]\n",
        "                curr_count = 0\n",
        "                curr_loss_gauss = 0.0\n",
        "                curr_loss_multi = 0.0\n",
        "\n",
        "            update_ema(self.ema_model.parameters(), self.diffusion._denoise_fn.parameters())\n",
        "\n",
        "            step += 1\n",
        "\n",
        "def train(\n",
        "    parent_dir,\n",
        "    real_data_path = 'data/higgs-small',\n",
        "    steps = 1000,\n",
        "    lr = 0.002,\n",
        "    weight_decay = 1e-4,\n",
        "    batch_size = 1024,\n",
        "    model_type = 'mlp',\n",
        "    model_params = None,\n",
        "    num_timesteps = 1000,\n",
        "    gaussian_loss_type = 'mse',\n",
        "    scheduler = 'cosine',\n",
        "    T_dict = None,\n",
        "    num_numerical_features = 0,\n",
        "    device = torch.device('cuda:1'),\n",
        "    seed = 0,\n",
        "    change_val = False\n",
        "):\n",
        "    real_data_path = os.path.normpath(real_data_path)\n",
        "    parent_dir = os.path.normpath(parent_dir)\n",
        "\n",
        "    def improve_reproducibility(seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    improve_reproducibility(seed)\n",
        "\n",
        "    T = lib.Transformations(**T_dict)\n",
        "\n",
        "    dataset = make_dataset(\n",
        "        real_data_path,\n",
        "        T,\n",
        "        num_classes=model_params['num_classes'],\n",
        "        is_y_cond=model_params['is_y_cond'],\n",
        "        change_val=change_val\n",
        "    )\n",
        "\n",
        "    K = np.array(dataset.get_category_sizes('train'))\n",
        "    if len(K) == 0 or T_dict['cat_encoding'] == 'one-hot':\n",
        "        K = np.array([0])\n",
        "    print(K)\n",
        "\n",
        "    num_numerical_features = dataset.X_num['train'].shape[1] if dataset.X_num is not None else 0\n",
        "    d_in = np.sum(K) + num_numerical_features\n",
        "    model_params['d_in'] = d_in\n",
        "    print(d_in)\n",
        "\n",
        "    print(model_params)\n",
        "    model = get_model(\n",
        "        model_type,\n",
        "        model_params,\n",
        "        num_numerical_features,\n",
        "        category_sizes=dataset.get_category_sizes('train')\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # train_loader = lib.prepare_beton_loader(dataset, split='train', batch_size=batch_size)\n",
        "    train_loader = lib.prepare_fast_dataloader(dataset, split='train', batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "    diffusion = GaussianMultinomialDiffusion(\n",
        "        num_classes=K,\n",
        "        num_numerical_features=num_numerical_features,\n",
        "        denoise_fn=model,\n",
        "        gaussian_loss_type=gaussian_loss_type,\n",
        "        num_timesteps=num_timesteps,\n",
        "        scheduler=scheduler,\n",
        "        device=device\n",
        "    )\n",
        "    diffusion.to(device)\n",
        "    diffusion.train()\n",
        "\n",
        "    trainer = Trainer(\n",
        "        diffusion,\n",
        "        train_loader,\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay,\n",
        "        steps=steps,\n",
        "        device=device\n",
        "    )\n",
        "    trainer.run_loop()\n",
        "\n",
        "    trainer.loss_history.to_csv(os.path.join(parent_dir, 'loss.csv'), index=False)\n",
        "    torch.save(diffusion._denoise_fn.state_dict(), os.path.join(parent_dir, 'model.pt'))\n",
        "    torch.save(trainer.ema_model.state_dict(), os.path.join(parent_dir, 'model_ema.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atSF8BV1c5nU",
        "outputId": "021bd454-80c1-4236-aa80-40c99aee2f18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lib/util.py\n",
        "import argparse\n",
        "import atexit\n",
        "import enum\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import shutil\n",
        "import sys\n",
        "import time\n",
        "import uuid\n",
        "from copy import deepcopy\n",
        "from dataclasses import asdict, fields, is_dataclass\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from typing import Any, Callable, List, Dict, Type, Optional, Tuple, TypeVar, Union, cast, get_args, get_origin\n",
        "\n",
        "import __main__\n",
        "import numpy as np\n",
        "import tomli\n",
        "import tomli_w\n",
        "import torch\n",
        "import zero\n",
        "\n",
        "from . import env\n",
        "\n",
        "RawConfig = Dict[str, Any]\n",
        "Report = Dict[str, Any]\n",
        "T = TypeVar('T')\n",
        "\n",
        "\n",
        "class Part(enum.Enum):\n",
        "    TRAIN = 'train'\n",
        "    VAL = 'val'\n",
        "    TEST = 'test'\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return self.value\n",
        "\n",
        "\n",
        "class TaskType(enum.Enum):\n",
        "    BINCLASS = 'binclass'\n",
        "    MULTICLASS = 'multiclass'\n",
        "    REGRESSION = 'regression'\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return self.value\n",
        "\n",
        "\n",
        "class Timer:\n",
        "    def __init__(self):\n",
        "        self.start_time = None\n",
        "        self.duration = 0\n",
        "\n",
        "    def run(self):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        if self.start_time is not None:\n",
        "            self.duration = time.time() - self.start_time\n",
        "            self.start_time = None\n",
        "        return self.duration\n",
        "\n",
        "    def get_duration(self):\n",
        "        return self.duration\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.duration:.2f} seconds\"\n",
        "\n",
        "    @classmethod\n",
        "    def launch(cls) -> 'Timer':\n",
        "        timer = cls()\n",
        "        timer.run()\n",
        "        return timer\n",
        "\n",
        "\n",
        "def update_training_log(training_log, data, metrics):\n",
        "    def _update(log_part, data_part):\n",
        "        for k, v in data_part.items():\n",
        "            if isinstance(v, dict):\n",
        "                _update(log_part.setdefault(k, {}), v)\n",
        "            elif isinstance(v, list):\n",
        "                log_part.setdefault(k, []).extend(v)\n",
        "            else:\n",
        "                log_part.setdefault(k, []).append(v)\n",
        "\n",
        "    _update(training_log, data)\n",
        "    transposed_metrics = {}\n",
        "    for part, part_metrics in metrics.items():\n",
        "        for metric_name, value in part_metrics.items():\n",
        "            transposed_metrics.setdefault(metric_name, {})[part] = value\n",
        "    _update(training_log, transposed_metrics)\n",
        "\n",
        "\n",
        "def raise_unknown(unknown_what: str, unknown_value: Any):\n",
        "    raise ValueError(f'Unknown {unknown_what}: {unknown_value}')\n",
        "\n",
        "\n",
        "def _replace(data, condition, value):\n",
        "    def do(x):\n",
        "        if isinstance(x, dict):\n",
        "            return {k: do(v) for k, v in x.items()}\n",
        "        elif isinstance(x, list):\n",
        "            return [do(y) for y in x]\n",
        "        else:\n",
        "            return value if condition(x) else x\n",
        "\n",
        "    return do(data)\n",
        "\n",
        "\n",
        "_CONFIG_NONE = '__none__'\n",
        "\n",
        "\n",
        "def unpack_config(config: RawConfig) -> RawConfig:\n",
        "    config = cast(RawConfig, _replace(config, lambda x: x == _CONFIG_NONE, None))\n",
        "    return config\n",
        "\n",
        "\n",
        "def pack_config(config: RawConfig) -> RawConfig:\n",
        "    config = cast(RawConfig, _replace(config, lambda x: x is None, _CONFIG_NONE))\n",
        "    return config\n",
        "\n",
        "\n",
        "def load_config(path: Union[Path, str]) -> Any:\n",
        "    with open(path, 'rb') as f:\n",
        "        return unpack_config(tomli.load(f))\n",
        "\n",
        "\n",
        "def dump_config(config: Any, path: Union[Path, str]) -> None:\n",
        "    with open(path, 'wb') as f:\n",
        "        tomli_w.dump(pack_config(config), f)\n",
        "    # check that there are no bugs in all these \"pack/unpack\" things\n",
        "    assert config == load_config(path)\n",
        "\n",
        "\n",
        "def load_json(path: Union[Path, str], **kwargs) -> Any:\n",
        "    return json.loads(Path(path).read_text(), **kwargs)\n",
        "\n",
        "\n",
        "def dump_json(x: Any, path: Union[Path, str], **kwargs) -> None:\n",
        "    kwargs.setdefault('indent', 4)\n",
        "    Path(path).write_text(json.dumps(x, **kwargs) + '\\n')\n",
        "\n",
        "\n",
        "def load_pickle(path: Union[Path, str], **kwargs) -> Any:\n",
        "    return pickle.loads(Path(path).read_bytes(), **kwargs)\n",
        "\n",
        "\n",
        "def dump_pickle(x: Any, path: Union[Path, str], **kwargs) -> None:\n",
        "    Path(path).write_bytes(pickle.dumps(x, **kwargs))\n",
        "\n",
        "\n",
        "def load(path: Union[Path, str], **kwargs) -> Any:\n",
        "    return globals()[f'load_{Path(path).suffix[1:]}'](Path(path), **kwargs)\n",
        "\n",
        "\n",
        "def dump(x: Any, path: Union[Path, str], **kwargs) -> Any:\n",
        "    return globals()[f'dump_{Path(path).suffix[1:]}'](x, Path(path), **kwargs)\n",
        "\n",
        "\n",
        "def _get_output_item_path(\n",
        "    path: Union[str, Path], filename: str, must_exist: bool\n",
        ") -> Path:\n",
        "    path = env.get_path(path)\n",
        "    if path.suffix == '.toml':\n",
        "        path = path.with_suffix('')\n",
        "    if path.is_dir():\n",
        "        path = path / filename\n",
        "    else:\n",
        "        assert path.name == filename\n",
        "    assert path.parent.exists()\n",
        "    if must_exist:\n",
        "        assert path.exists()\n",
        "    return path\n",
        "\n",
        "\n",
        "def load_report(path: Path) -> Report:\n",
        "    return load_json(_get_output_item_path(path, 'report.json', True))\n",
        "\n",
        "\n",
        "def dump_report(report: dict, path: Path) -> None:\n",
        "    dump_json(report, _get_output_item_path(path, 'report.json', False))\n",
        "\n",
        "\n",
        "def load_predictions(path: Path) -> Dict[str, np.ndarray]:\n",
        "    with np.load(_get_output_item_path(path, 'predictions.npz', True)) as predictions:\n",
        "        return {x: predictions[x] for x in predictions}\n",
        "\n",
        "\n",
        "def dump_predictions(predictions: Dict[str, np.ndarray], path: Path) -> None:\n",
        "    np.savez(_get_output_item_path(path, 'predictions.npz', False), **predictions)\n",
        "\n",
        "\n",
        "def dump_metrics(metrics: Dict[str, Any], path: Path) -> None:\n",
        "    dump_json(metrics, _get_output_item_path(path, 'metrics.json', False))\n",
        "\n",
        "\n",
        "def load_checkpoint(path: Path, *args, **kwargs) -> Dict[str, np.ndarray]:\n",
        "    return torch.load(\n",
        "        _get_output_item_path(path, 'checkpoint.pt', True), *args, **kwargs\n",
        "    )\n",
        "\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    if torch.cuda.is_available():\n",
        "        assert os.environ.get('CUDA_VISIBLE_DEVICES') is not None\n",
        "        return torch.device('cuda:0')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "def _print_sep(c, size=100):\n",
        "    print(c * size)\n",
        "\n",
        "\n",
        "def start(\n",
        "    config_cls: Type[T] = RawConfig,\n",
        "    argv: Optional[List[str]] = None,\n",
        "    patch_raw_config: Optional[Callable[[RawConfig], None]] = None,\n",
        ") -> Tuple[T, Path, Report]:  # config  # output dir  # report\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('config', metavar='FILE')\n",
        "    parser.add_argument('--force', action='store_true')\n",
        "    parser.add_argument('--continue', action='store_true', dest='continue_')\n",
        "    if argv is None:\n",
        "        program = __main__.__file__\n",
        "        args = parser.parse_args()\n",
        "    else:\n",
        "        program = argv[0]\n",
        "        try:\n",
        "            args = parser.parse_args(argv[1:])\n",
        "        except Exception:\n",
        "            print(\n",
        "                'Failed to parse `argv`.'\n",
        "                ' Remember that the first item of `argv` must be the path (relative to'\n",
        "                ' the project root) to the script/notebook.'\n",
        "            )\n",
        "            raise\n",
        "    args = parser.parse_args(argv)\n",
        "\n",
        "    snapshot_dir = os.environ.get('SNAPSHOT_PATH')\n",
        "    if snapshot_dir and Path(snapshot_dir).joinpath('CHECKPOINTS_RESTORED').exists():\n",
        "        assert args.continue_\n",
        "\n",
        "    config_path = env.get_path(args.config)\n",
        "    output_dir = config_path.with_suffix('')\n",
        "    _print_sep('=')\n",
        "    print(f'[output] {output_dir}')\n",
        "    _print_sep('=')\n",
        "\n",
        "    assert config_path.exists()\n",
        "    raw_config = load_config(config_path)\n",
        "    if patch_raw_config is not None:\n",
        "        patch_raw_config(raw_config)\n",
        "    if is_dataclass(config_cls):\n",
        "        config = from_dict(config_cls, raw_config)\n",
        "        full_raw_config = asdict(config)\n",
        "    else:\n",
        "        assert config_cls is dict\n",
        "        full_raw_config = config = raw_config\n",
        "    full_raw_config = asdict(config)\n",
        "\n",
        "    if output_dir.exists():\n",
        "        if args.force:\n",
        "            print('Removing the existing output and creating a new one...')\n",
        "            shutil.rmtree(output_dir)\n",
        "            output_dir.mkdir()\n",
        "        elif not args.continue_:\n",
        "            backup_output(output_dir)\n",
        "            print('The output directory already exists. Done!\\n')\n",
        "            sys.exit()\n",
        "        elif output_dir.joinpath('DONE').exists():\n",
        "            backup_output(output_dir)\n",
        "            print('The \"DONE\" file already exists. Done!')\n",
        "            sys.exit()\n",
        "        else:\n",
        "            print('Continuing with the existing output...')\n",
        "    else:\n",
        "        print('Creating the output...')\n",
        "        output_dir.mkdir()\n",
        "\n",
        "    report = {\n",
        "        'program': str(env.get_relative_path(program)),\n",
        "        'environment': {},\n",
        "        'config': full_raw_config,\n",
        "    }\n",
        "    if torch.cuda.is_available():  # type: ignore[code]\n",
        "        report['environment'].update(\n",
        "            {\n",
        "                'CUDA_VISIBLE_DEVICES': os.environ.get('CUDA_VISIBLE_DEVICES'),\n",
        "                'gpus': zero.hardware.get_gpus_info(),\n",
        "                'torch.version.cuda': torch.version.cuda,\n",
        "                'torch.backends.cudnn.version()': torch.backends.cudnn.version(),  # type: ignore[code]\n",
        "                'torch.cuda.nccl.version()': torch.cuda.nccl.version(),  # type: ignore[code]\n",
        "            }\n",
        "        )\n",
        "    dump_report(report, output_dir)\n",
        "    dump_json(raw_config, output_dir / 'raw_config.json')\n",
        "    _print_sep('-')\n",
        "    pprint(full_raw_config, width=100)\n",
        "    _print_sep('-')\n",
        "    return cast(config_cls, config), output_dir, report\n",
        "\n",
        "\n",
        "_LAST_SNAPSHOT_TIME = None\n",
        "\n",
        "\n",
        "def backup_output(output_dir: Path) -> None:\n",
        "    backup_dir = os.environ.get('TMP_OUTPUT_PATH')\n",
        "    snapshot_dir = os.environ.get('SNAPSHOT_PATH')\n",
        "    if backup_dir is None:\n",
        "        assert snapshot_dir is None\n",
        "        return\n",
        "    assert snapshot_dir is not None\n",
        "\n",
        "    try:\n",
        "        relative_output_dir = output_dir.relative_to(env.PROJ)\n",
        "    except ValueError:\n",
        "        return\n",
        "\n",
        "    for dir_ in [backup_dir, snapshot_dir]:\n",
        "        new_output_dir = dir_ / relative_output_dir\n",
        "        prev_backup_output_dir = new_output_dir.with_name(new_output_dir.name + '_prev')\n",
        "        new_output_dir.parent.mkdir(exist_ok=True, parents=True)\n",
        "        if new_output_dir.exists():\n",
        "            new_output_dir.rename(prev_backup_output_dir)\n",
        "        shutil.copytree(output_dir, new_output_dir)\n",
        "        # the case for evaluate.py which automatically creates configs\n",
        "        if output_dir.with_suffix('.toml').exists():\n",
        "            shutil.copyfile(\n",
        "                output_dir.with_suffix('.toml'), new_output_dir.with_suffix('.toml')\n",
        "            )\n",
        "        if prev_backup_output_dir.exists():\n",
        "            shutil.rmtree(prev_backup_output_dir)\n",
        "\n",
        "    global _LAST_SNAPSHOT_TIME\n",
        "    if _LAST_SNAPSHOT_TIME is None or time.time() - _LAST_SNAPSHOT_TIME > 10 * 60:\n",
        "        import nirvana_dl.snapshot  # type: ignore[code]\n",
        "\n",
        "        nirvana_dl.snapshot.dump_snapshot()\n",
        "        _LAST_SNAPSHOT_TIME = time.time()\n",
        "        print('The snapshot was saved!')\n",
        "\n",
        "\n",
        "def _get_scores(metrics: Dict[str, Dict[str, Any]]) -> Optional[Dict[str, float]]:\n",
        "    return (\n",
        "        {k: v['score'] for k, v in metrics.items()}\n",
        "        if 'score' in next(iter(metrics.values()))\n",
        "        else None\n",
        "    )\n",
        "\n",
        "\n",
        "def format_scores(metrics: Dict[str, Dict[str, Any]]) -> str:\n",
        "    return ' '.join(\n",
        "        f\"[{x}] {metrics[x]['score']:.3f}\"\n",
        "        for x in ['test', 'val', 'train']\n",
        "        if x in metrics\n",
        "    )\n",
        "\n",
        "\n",
        "def finish(output_dir: Path, report: dict) -> None:\n",
        "    print()\n",
        "    _print_sep('=')\n",
        "\n",
        "    metrics = report.get('metrics')\n",
        "    if metrics is not None:\n",
        "        scores = _get_scores(metrics)\n",
        "        if scores is not None:\n",
        "            dump_json(scores, output_dir / 'scores.json')\n",
        "            print(format_scores(metrics))\n",
        "            _print_sep('-')\n",
        "\n",
        "    dump_report(report, output_dir)\n",
        "    json_output_path = os.environ.get('JSON_OUTPUT_FILE')\n",
        "    if json_output_path:\n",
        "        try:\n",
        "            key = str(output_dir.relative_to(env.PROJ))\n",
        "        except ValueError:\n",
        "            pass\n",
        "        else:\n",
        "            json_output_path = Path(json_output_path)\n",
        "            try:\n",
        "                json_data = json.loads(json_output_path.read_text())\n",
        "            except (FileNotFoundError, json.decoder.JSONDecodeError):\n",
        "                json_data = {}\n",
        "            json_data[key] = load_json(output_dir / 'report.json')\n",
        "            json_output_path.write_text(json.dumps(json_data, indent=4))\n",
        "        shutil.copyfile(\n",
        "            json_output_path,\n",
        "            os.path.join(os.environ['SNAPSHOT_PATH'], 'json_output.json'),\n",
        "        )\n",
        "\n",
        "    output_dir.joinpath('DONE').touch()\n",
        "    backup_output(output_dir)\n",
        "    print(f'Done! | {report.get(\"time\")} | {output_dir}')\n",
        "    _print_sep('=')\n",
        "    print()\n",
        "\n",
        "\n",
        "def from_dict(datacls: Type[T], data: dict) -> T:\n",
        "    assert is_dataclass(datacls)\n",
        "    data = deepcopy(data)\n",
        "    for field in fields(datacls):\n",
        "        if field.name not in data:\n",
        "            continue\n",
        "        if is_dataclass(field.type):\n",
        "            data[field.name] = from_dict(field.type, data[field.name])\n",
        "        elif (\n",
        "            get_origin(field.type) is Union\n",
        "            and len(get_args(field.type)) == 2\n",
        "            and get_args(field.type)[1] is type(None)\n",
        "            and is_dataclass(get_args(field.type)[0])\n",
        "        ):\n",
        "            if data[field.name] is not None:\n",
        "                data[field.name] = from_dict(get_args(field.type)[0], data[field.name])\n",
        "    return datacls(**data)\n",
        "\n",
        "\n",
        "def replace_factor_with_value(\n",
        "    config: RawConfig,\n",
        "    key: str,\n",
        "    reference_value: int,\n",
        "    bounds: Tuple[float, float],\n",
        ") -> None:\n",
        "    factor_key = key + '_factor'\n",
        "    if factor_key not in config:\n",
        "        assert key in config\n",
        "    else:\n",
        "        assert key not in config\n",
        "        factor = config.pop(factor_key)\n",
        "        assert bounds[0] <= factor <= bounds[1]\n",
        "        config[key] = int(factor * reference_value)\n",
        "\n",
        "\n",
        "def get_temporary_copy(path: Union[str, Path]) -> Path:\n",
        "    path = env.get_path(path)\n",
        "    assert not path.is_dir() and not path.is_symlink()\n",
        "    tmp_path = path.with_name(\n",
        "        path.stem + '___' + str(uuid.uuid4()).replace('-', '') + path.suffix\n",
        "    )\n",
        "    shutil.copyfile(path, tmp_path)\n",
        "    atexit.register(lambda: tmp_path.unlink())\n",
        "    return tmp_path\n",
        "\n",
        "\n",
        "def get_python():\n",
        "    python = Path('python3.9')\n",
        "    return str(python) if python.exists() else 'python'\n",
        "\n",
        "def get_catboost_config(real_data_path, is_cv=False):\n",
        "    ds_name = Path(real_data_path).name\n",
        "    C = load_json(f'tuned_models/catboost/{ds_name}_cv.json')\n",
        "    return C"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJdF2SaAeFeg",
        "outputId": "e157fa9f-8711-4558-831e-cbefab515f09"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting lib/util.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/pipeline.py\n",
        "import tomli\n",
        "import shutil\n",
        "import os\n",
        "import argparse\n",
        "from train import train\n",
        "from sample import sample\n",
        "from eval_catboost import train_catboost\n",
        "from eval_mlp import train_mlp\n",
        "from eval_simple import train_simple\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import zero\n",
        "import lib\n",
        "import torch\n",
        "\n",
        "import sys\n",
        "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n",
        "from lib.util import Timer\n",
        "\n",
        "\n",
        "def load_config(path) :\n",
        "    with open(path, 'rb') as f:\n",
        "        return tomli.load(f)\n",
        "\n",
        "def save_file(parent_dir, config_path):\n",
        "    try:\n",
        "        dst = os.path.join(parent_dir)\n",
        "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "        shutil.copyfile(os.path.abspath(config_path), dst)\n",
        "    except shutil.SameFileError:\n",
        "        pass\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', metavar='FILE')\n",
        "    parser.add_argument('--train', action='store_true', default=False)\n",
        "    parser.add_argument('--sample', action='store_true',  default=False)\n",
        "    parser.add_argument('--eval', action='store_true',  default=False)\n",
        "    parser.add_argument('--change_val', action='store_true',  default=False)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    raw_config = lib.load_config(args.config)\n",
        "    if 'device' in raw_config:\n",
        "        device = torch.device(raw_config['device'])\n",
        "    else:\n",
        "        device = torch.device('cuda:1')\n",
        "\n",
        "    timer = Timer()\n",
        "    timer.run()\n",
        "    save_file(os.path.join(raw_config['parent_dir'], 'config.toml'), args.config)\n",
        "\n",
        "    if args.train:\n",
        "        train(\n",
        "            **raw_config['train']['main'],\n",
        "            **raw_config['diffusion_params'],\n",
        "            parent_dir=raw_config['parent_dir'],\n",
        "            real_data_path=raw_config['real_data_path'],\n",
        "            model_type=raw_config['model_type'],\n",
        "            model_params=raw_config['model_params'],\n",
        "            T_dict=raw_config['train']['T'],\n",
        "            num_numerical_features=raw_config['num_numerical_features'],\n",
        "            device=device,\n",
        "            change_val=args.change_val\n",
        "        )\n",
        "    if args.sample:\n",
        "        sample(\n",
        "            num_samples=raw_config['sample']['num_samples'],\n",
        "            batch_size=raw_config['sample']['batch_size'],\n",
        "            disbalance=raw_config['sample'].get('disbalance', None),\n",
        "            **raw_config['diffusion_params'],\n",
        "            parent_dir=raw_config['parent_dir'],\n",
        "            real_data_path=raw_config['real_data_path'],\n",
        "            model_path=os.path.join(raw_config['parent_dir'], 'model.pt'),\n",
        "            model_type=raw_config['model_type'],\n",
        "            model_params=raw_config['model_params'],\n",
        "            T_dict=raw_config['train']['T'],\n",
        "            num_numerical_features=raw_config['num_numerical_features'],\n",
        "            device=device,\n",
        "            seed=raw_config['sample'].get('seed', 0),\n",
        "            change_val=args.change_val\n",
        "        )\n",
        "\n",
        "    save_file(os.path.join(raw_config['parent_dir'], 'info.json'), os.path.join(raw_config['real_data_path'], 'info.json'))\n",
        "    if args.eval:\n",
        "        if raw_config['eval']['type']['eval_model'] == 'catboost':\n",
        "            train_catboost(\n",
        "                parent_dir=raw_config['parent_dir'],\n",
        "                real_data_path=raw_config['real_data_path'],\n",
        "                eval_type=raw_config['eval']['type']['eval_type'],\n",
        "                T_dict=raw_config['eval']['T'],\n",
        "                seed=raw_config['seed'],\n",
        "                change_val=args.change_val\n",
        "            )\n",
        "        elif raw_config['eval']['type']['eval_model'] == 'mlp':\n",
        "            train_mlp(\n",
        "                parent_dir=raw_config['parent_dir'],\n",
        "                real_data_path=raw_config['real_data_path'],\n",
        "                eval_type=raw_config['eval']['type']['eval_type'],\n",
        "                T_dict=raw_config['eval']['T'],\n",
        "                seed=raw_config['seed'],\n",
        "                change_val=args.change_val,\n",
        "                device=device\n",
        "            )\n",
        "        elif raw_config['eval']['type']['eval_model'] == 'simple':\n",
        "            train_simple(\n",
        "                parent_dir=raw_config['parent_dir'],\n",
        "                real_data_path=raw_config['real_data_path'],\n",
        "                eval_type=raw_config['eval']['type']['eval_type'],\n",
        "                T_dict=raw_config['eval']['T'],\n",
        "                seed=raw_config['seed'],\n",
        "                change_val=args.change_val\n",
        "            )\n",
        "\n",
        "    print(f'Elapsed time: {str(timer)}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz94g-pYhahv",
        "outputId": "0290b2cf-c7ae-414d-d0eb-a2eade66b9c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lib/data.py\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "from copy import deepcopy\n",
        "from dataclasses import astuple, dataclass, replace\n",
        "from importlib.resources import path\n",
        "from pathlib import Path\n",
        "from typing import Any, Literal, Optional, Union, cast, Tuple, Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import sklearn.preprocessing\n",
        "import torch\n",
        "import os\n",
        "from category_encoders import LeaveOneOutEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "from . import env, util\n",
        "from .metrics import calculate_metrics as calculate_metrics_\n",
        "from .util import TaskType, load_json\n",
        "\n",
        "ArrayDict = Dict[str, np.ndarray]\n",
        "TensorDict = Dict[str, torch.Tensor]\n",
        "\n",
        "\n",
        "CAT_MISSING_VALUE = '__nan__'\n",
        "CAT_RARE_VALUE = '__rare__'\n",
        "Normalization = Literal['standard', 'quantile', 'minmax']\n",
        "NumNanPolicy = Literal['drop-rows', 'mean']\n",
        "CatNanPolicy = Literal['most_frequent']\n",
        "CatEncoding = Literal['one-hot', 'counter']\n",
        "YPolicy = Literal['default']\n",
        "\n",
        "\n",
        "class StandardScaler1d(StandardScaler):\n",
        "    def partial_fit(self, X, *args, **kwargs):\n",
        "        assert X.ndim == 1\n",
        "        return super().partial_fit(X[:, None], *args, **kwargs)\n",
        "\n",
        "    def transform(self, X, *args, **kwargs):\n",
        "        assert X.ndim == 1\n",
        "        return super().transform(X[:, None], *args, **kwargs).squeeze(1)\n",
        "\n",
        "    def inverse_transform(self, X, *args, **kwargs):\n",
        "        assert X.ndim == 1\n",
        "        return super().inverse_transform(X[:, None], *args, **kwargs).squeeze(1)\n",
        "\n",
        "\n",
        "def get_category_sizes(X: Union[torch.Tensor, np.ndarray]) -> List[int]:\n",
        "    XT = X.T.cpu().tolist() if isinstance(X, torch.Tensor) else X.T.tolist()\n",
        "    return [len(set(x)) for x in XT]\n",
        "\n",
        "\n",
        "@dataclass(frozen=False)\n",
        "class Dataset:\n",
        "    X_num: Optional[ArrayDict]\n",
        "    X_cat: Optional[ArrayDict]\n",
        "    y: ArrayDict\n",
        "    y_info: Dict[str, Any]\n",
        "    task_type: TaskType\n",
        "    n_classes: Optional[int]\n",
        "\n",
        "    @classmethod\n",
        "    def from_dir(cls, dir_: Union[Path, str]) -> 'Dataset':\n",
        "        dir_ = Path(dir_)\n",
        "        splits = [k for k in ['train', 'val', 'test'] if dir_.joinpath(f'y_{k}.npy').exists()]\n",
        "\n",
        "        def load(item) -> ArrayDict:\n",
        "            return {\n",
        "                x: cast(np.ndarray, np.load(dir_ / f'{item}_{x}.npy', allow_pickle=True))  # type: ignore[code]\n",
        "                for x in splits\n",
        "            }\n",
        "\n",
        "        if Path(dir_ / 'info.json').exists():\n",
        "            info = util.load_json(dir_ / 'info.json')\n",
        "        else:\n",
        "            info = None\n",
        "        return Dataset(\n",
        "            load('X_num') if dir_.joinpath('X_num_train.npy').exists() else None,\n",
        "            load('X_cat') if dir_.joinpath('X_cat_train.npy').exists() else None,\n",
        "            load('y'),\n",
        "            {},\n",
        "            TaskType(info['task_type']),\n",
        "            info.get('n_classes'),\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def is_binclass(self) -> bool:\n",
        "        return self.task_type == TaskType.BINCLASS\n",
        "\n",
        "    @property\n",
        "    def is_multiclass(self) -> bool:\n",
        "        return self.task_type == TaskType.MULTICLASS\n",
        "\n",
        "    @property\n",
        "    def is_regression(self) -> bool:\n",
        "        return self.task_type == TaskType.REGRESSION\n",
        "\n",
        "    @property\n",
        "    def n_num_features(self) -> int:\n",
        "        return 0 if self.X_num is None else self.X_num['train'].shape[1]\n",
        "\n",
        "    @property\n",
        "    def n_cat_features(self) -> int:\n",
        "        return 0 if self.X_cat is None else self.X_cat['train'].shape[1]\n",
        "\n",
        "    @property\n",
        "    def n_features(self) -> int:\n",
        "        return self.n_num_features + self.n_cat_features\n",
        "\n",
        "    def size(self, part: Optional[str]) -> int:\n",
        "        return sum(map(len, self.y.values())) if part is None else len(self.y[part])\n",
        "\n",
        "    @property\n",
        "    def nn_output_dim(self) -> int:\n",
        "        if self.is_multiclass:\n",
        "            assert self.n_classes is not None\n",
        "            return self.n_classes\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    def get_category_sizes(self, part: str) -> List[int]:\n",
        "        return [] if self.X_cat is None else get_category_sizes(self.X_cat[part])\n",
        "\n",
        "    def calculate_metrics(\n",
        "        self,\n",
        "        predictions: Dict[str, np.ndarray],\n",
        "        prediction_type: Optional[str],\n",
        "    ) -> Dict[str, Any]:\n",
        "        metrics = {\n",
        "            x: calculate_metrics_(\n",
        "                self.y[x], predictions[x], self.task_type, prediction_type, self.y_info\n",
        "            )\n",
        "            for x in predictions\n",
        "        }\n",
        "        if self.task_type == TaskType.REGRESSION:\n",
        "            score_key = 'rmse'\n",
        "            score_sign = -1\n",
        "        else:\n",
        "            score_key = 'accuracy'\n",
        "            score_sign = 1\n",
        "        for part_metrics in metrics.values():\n",
        "            part_metrics['score'] = score_sign * part_metrics[score_key]\n",
        "        return metrics\n",
        "\n",
        "def change_val(dataset: Dataset, val_size: float = 0.2):\n",
        "    # should be done before transformations\n",
        "\n",
        "    y = np.concatenate([dataset.y['train'], dataset.y['val']], axis=0)\n",
        "\n",
        "    ixs = np.arange(y.shape[0])\n",
        "    if dataset.is_regression:\n",
        "        train_ixs, val_ixs = train_test_split(ixs, test_size=val_size, random_state=777)\n",
        "    else:\n",
        "        train_ixs, val_ixs = train_test_split(ixs, test_size=val_size, random_state=777, stratify=y)\n",
        "\n",
        "    dataset.y['train'] = y[train_ixs]\n",
        "    dataset.y['val'] = y[val_ixs]\n",
        "\n",
        "    if dataset.X_num is not None:\n",
        "        X_num = np.concatenate([dataset.X_num['train'], dataset.X_num['val']], axis=0)\n",
        "        dataset.X_num['train'] = X_num[train_ixs]\n",
        "        dataset.X_num['val'] = X_num[val_ixs]\n",
        "\n",
        "    if dataset.X_cat is not None:\n",
        "        X_cat = np.concatenate([dataset.X_cat['train'], dataset.X_cat['val']], axis=0)\n",
        "        dataset.X_cat['train'] = X_cat[train_ixs]\n",
        "        dataset.X_cat['val'] = X_cat[val_ixs]\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def num_process_nans(dataset: Dataset, policy: Optional[NumNanPolicy]) -> Dataset:\n",
        "    assert dataset.X_num is not None\n",
        "\n",
        "    print(\"\\n[DEBUG] Inspecting dataset.X_num types before np.isnan:\")\n",
        "    for k, v in dataset.X_num.items():\n",
        "        print(f\"  - X_num['{k}']: type = {type(v)}, dtype = {v.dtype}, shape = {v.shape}\")\n",
        "        print(f\"    first row: {v[0]}\")\n",
        "\n",
        "    nan_masks = {k: np.isnan(v) for k, v in dataset.X_num.items()}\n",
        "    if not any(x.any() for x in nan_masks.values()):  # type: ignore[code]\n",
        "        assert policy is None\n",
        "        return dataset\n",
        "\n",
        "    assert policy is not None\n",
        "    if policy == 'drop-rows':\n",
        "        valid_masks = {k: ~v.any(1) for k, v in nan_masks.items()}\n",
        "        assert valid_masks[\n",
        "            'test'\n",
        "        ].all(), 'Cannot drop test rows, since this will affect the final metrics.'\n",
        "        new_data = {}\n",
        "        for data_name in ['X_num', 'X_cat', 'y']:\n",
        "            data_dict = getattr(dataset, data_name)\n",
        "            if data_dict is not None:\n",
        "                new_data[data_name] = {\n",
        "                    k: v[valid_masks[k]] for k, v in data_dict.items()\n",
        "                }\n",
        "        dataset = replace(dataset, **new_data)\n",
        "    elif policy == 'mean':\n",
        "        new_values = np.nanmean(dataset.X_num['train'], axis=0)\n",
        "        X_num = deepcopy(dataset.X_num)\n",
        "        for k, v in X_num.items():\n",
        "            num_nan_indices = np.where(nan_masks[k])\n",
        "            v[num_nan_indices] = np.take(new_values, num_nan_indices[1])\n",
        "        dataset = replace(dataset, X_num=X_num)\n",
        "    else:\n",
        "        assert util.raise_unknown('policy', policy)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Inspired by: https://github.com/yandex-research/rtdl/blob/a4c93a32b334ef55d2a0559a4407c8306ffeeaee/lib/data.py#L20\n",
        "def normalize(\n",
        "    X: ArrayDict, normalization: Normalization, seed: Optional[int], return_normalizer : bool = False\n",
        ") -> ArrayDict:\n",
        "    X_train = X['train']\n",
        "    if normalization == 'standard':\n",
        "        normalizer = sklearn.preprocessing.StandardScaler()\n",
        "    elif normalization == 'minmax':\n",
        "        normalizer = sklearn.preprocessing.MinMaxScaler()\n",
        "    elif normalization == 'quantile':\n",
        "        normalizer = sklearn.preprocessing.QuantileTransformer(\n",
        "            output_distribution='normal',\n",
        "            n_quantiles=max(min(X['train'].shape[0] // 30, 1000), 10),\n",
        "            subsample=1e9,\n",
        "            random_state=seed,\n",
        "        )\n",
        "        # noise = 1e-3\n",
        "        # if noise > 0:\n",
        "        #     assert seed is not None\n",
        "        #     stds = np.std(X_train, axis=0, keepdims=True)\n",
        "        #     noise_std = noise / np.maximum(stds, noise)  # type: ignore[code]\n",
        "        #     X_train = X_train + noise_std * np.random.default_rng(seed).standard_normal(\n",
        "        #         X_train.shape\n",
        "        #     )\n",
        "    else:\n",
        "        util.raise_unknown('normalization', normalization)\n",
        "    normalizer.fit(X_train)\n",
        "    if return_normalizer:\n",
        "        return {k: normalizer.transform(v) for k, v in X.items()}, normalizer\n",
        "    return {k: normalizer.transform(v) for k, v in X.items()}\n",
        "\n",
        "\n",
        "def cat_process_nans(X: ArrayDict, policy: Optional[CatNanPolicy]) -> ArrayDict:\n",
        "    assert X is not None\n",
        "    X = {k: v.astype(str) for k, v in X.items()}\n",
        "    nan_masks = {\n",
        "        k: np.equal(v, CAT_MISSING_VALUE) if isinstance(v, np.ndarray) else np.array([False])\n",
        "        for k, v in X.items()\n",
        "    }\n",
        "    if any(np.any(x) for x in nan_masks.values()):\n",
        "        if policy is None:\n",
        "            X_new = X\n",
        "        elif policy == 'most_frequent':\n",
        "            imputer = SimpleImputer(missing_values=CAT_MISSING_VALUE, strategy=policy)  # type: ignore[code]\n",
        "            imputer.fit(X['train'])\n",
        "            X_new = {k: cast(np.ndarray, imputer.transform(v)) for k, v in X.items()}\n",
        "        else:\n",
        "            util.raise_unknown('categorical NaN policy', policy)\n",
        "    else:\n",
        "        assert policy is None\n",
        "        X_new = X\n",
        "    return X_new\n",
        "\n",
        "\n",
        "def cat_drop_rare(X: ArrayDict, min_frequency: float) -> ArrayDict:\n",
        "    assert 0.0 < min_frequency < 1.0\n",
        "    min_count = round(len(X['train']) * min_frequency)\n",
        "    X_new = {x: [] for x in X}\n",
        "    for column_idx in range(X['train'].shape[1]):\n",
        "        counter = Counter(X['train'][:, column_idx].tolist())\n",
        "        popular_categories = {k for k, v in counter.items() if v >= min_count}\n",
        "        for part in X_new:\n",
        "            X_new[part].append(\n",
        "                [\n",
        "                    (x if x in popular_categories else CAT_RARE_VALUE)\n",
        "                    for x in X[part][:, column_idx].tolist()\n",
        "                ]\n",
        "            )\n",
        "    return {k: np.array(v).T for k, v in X_new.items()}\n",
        "\n",
        "\n",
        "def cat_encode(\n",
        "    X: ArrayDict,\n",
        "    encoding: Optional[CatEncoding],\n",
        "    y_train: Optional[np.ndarray],\n",
        "    seed: Optional[int],\n",
        "    return_encoder : bool = False\n",
        ") -> Tuple[ArrayDict, bool, Optional[Any]]:  # (X, is_converted_to_numerical)\n",
        "    if encoding != 'counter':\n",
        "        y_train = None\n",
        "\n",
        "    # Step 1. Map strings to 0-based ranges\n",
        "\n",
        "    if encoding is None:\n",
        "        unknown_value = np.iinfo('int64').max - 3\n",
        "        oe = sklearn.preprocessing.OrdinalEncoder(\n",
        "            handle_unknown='use_encoded_value',  # type: ignore[code]\n",
        "            unknown_value=unknown_value,  # type: ignore[code]\n",
        "            dtype='int64',  # type: ignore[code]\n",
        "        ).fit(X['train'])\n",
        "        encoder = make_pipeline(oe)\n",
        "        encoder.fit(X['train'])\n",
        "        X = {k: encoder.transform(v) for k, v in X.items()}\n",
        "        max_values = X['train'].max(axis=0)\n",
        "        for part in X.keys():\n",
        "            if part == 'train': continue\n",
        "            for column_idx in range(X[part].shape[1]):\n",
        "                X[part][X[part][:, column_idx] == unknown_value, column_idx] = (\n",
        "                    max_values[column_idx] + 1\n",
        "                )\n",
        "        if return_encoder:\n",
        "            return (X, False, encoder)\n",
        "        return (X, False)\n",
        "\n",
        "    # Step 2. Encode.\n",
        "\n",
        "    elif encoding == 'one-hot':\n",
        "        ohe = sklearn.preprocessing.OneHotEncoder(\n",
        "            handle_unknown='ignore', sparse=False, dtype=np.float32 # type: ignore[code]\n",
        "        )\n",
        "        encoder = make_pipeline(ohe)\n",
        "\n",
        "        # encoder.steps.append(('ohe', ohe))\n",
        "        encoder.fit(X['train'])\n",
        "        X = {k: encoder.transform(v) for k, v in X.items()}\n",
        "    elif encoding == 'counter':\n",
        "        assert y_train is not None\n",
        "        assert seed is not None\n",
        "        loe = LeaveOneOutEncoder(sigma=0.1, random_state=seed, return_df=False)\n",
        "        encoder.steps.append(('loe', loe))\n",
        "        encoder.fit(X['train'], y_train)\n",
        "        X = {k: encoder.transform(v).astype('float32') for k, v in X.items()}  # type: ignore[code]\n",
        "        if not isinstance(X['train'], pd.DataFrame):\n",
        "            X = {k: v.values for k, v in X.items()}  # type: ignore[code]\n",
        "    else:\n",
        "        util.raise_unknown('encoding', encoding)\n",
        "\n",
        "    if return_encoder:\n",
        "        return X, True, encoder # type: ignore[code]\n",
        "    return (X, True)\n",
        "\n",
        "\n",
        "def build_target(\n",
        "    y: ArrayDict, policy: Optional[YPolicy], task_type: TaskType\n",
        ") -> Tuple[ArrayDict, Dict[str, Any]]:\n",
        "    info: Dict[str, Any] = {'policy': policy}\n",
        "    if policy is None:\n",
        "        pass\n",
        "    elif policy == 'default':\n",
        "        if task_type == TaskType.REGRESSION:\n",
        "            mean, std = float(y['train'].mean()), float(y['train'].std())\n",
        "            y = {k: (v - mean) / std for k, v in y.items()}\n",
        "            info['mean'] = mean\n",
        "            info['std'] = std\n",
        "    else:\n",
        "        util.raise_unknown('policy', policy)\n",
        "    return y, info\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Transformations:\n",
        "    seed: int = 0\n",
        "    normalization: Optional[Normalization] = None\n",
        "    num_nan_policy: Optional[NumNanPolicy] = None\n",
        "    cat_nan_policy: Optional[CatNanPolicy] = None\n",
        "    cat_min_frequency: Optional[float] = None\n",
        "    cat_encoding: Optional[CatEncoding] = None\n",
        "    y_policy: Optional[YPolicy] = 'default'\n",
        "\n",
        "\n",
        "def transform_dataset(\n",
        "    dataset: Dataset,\n",
        "    transformations: Transformations,\n",
        "    cache_dir: Optional[Path],\n",
        "    return_transforms: bool = False\n",
        ") -> Dataset:\n",
        "    # WARNING: the order of transformations matters. Moreover, the current\n",
        "    # implementation is not ideal in that sense.\n",
        "    if cache_dir is not None:\n",
        "        transformations_md5 = hashlib.md5(\n",
        "            str(transformations).encode('utf-8')\n",
        "        ).hexdigest()\n",
        "        transformations_str = '__'.join(map(str, astuple(transformations)))\n",
        "        cache_path = (\n",
        "            cache_dir / f'cache__{transformations_str}__{transformations_md5}.pickle'\n",
        "        )\n",
        "        if cache_path.exists():\n",
        "            cache_transformations, value = util.load_pickle(cache_path)\n",
        "            if transformations == cache_transformations:\n",
        "                print(\n",
        "                    f\"Using cached features: {cache_dir.name + '/' + cache_path.name}\"\n",
        "                )\n",
        "                return value\n",
        "            else:\n",
        "                raise RuntimeError(f'Hash collision for {cache_path}')\n",
        "    else:\n",
        "        cache_path = None\n",
        "\n",
        "    if dataset.X_num is not None:\n",
        "        dataset = num_process_nans(dataset, transformations.num_nan_policy)\n",
        "\n",
        "    num_transform = None\n",
        "    cat_transform = None\n",
        "    X_num = dataset.X_num\n",
        "\n",
        "    if X_num is not None and transformations.normalization is not None:\n",
        "        X_num, num_transform = normalize(\n",
        "            X_num,\n",
        "            transformations.normalization,\n",
        "            transformations.seed,\n",
        "            return_normalizer=True\n",
        "        )\n",
        "        num_transform = num_transform\n",
        "\n",
        "    if dataset.X_cat is None:\n",
        "        assert transformations.cat_nan_policy is None\n",
        "        assert transformations.cat_min_frequency is None\n",
        "        # assert transformations.cat_encoding is None\n",
        "        X_cat = None\n",
        "    else:\n",
        "        X_cat = cat_process_nans(dataset.X_cat, transformations.cat_nan_policy)\n",
        "        if transformations.cat_min_frequency is not None:\n",
        "            X_cat = cat_drop_rare(X_cat, transformations.cat_min_frequency)\n",
        "        X_cat, is_num, cat_transform = cat_encode(\n",
        "            X_cat,\n",
        "            transformations.cat_encoding,\n",
        "            dataset.y['train'],\n",
        "            transformations.seed,\n",
        "            return_encoder=True\n",
        "        )\n",
        "        if is_num:\n",
        "            X_num = (\n",
        "                X_cat\n",
        "                if X_num is None\n",
        "                else {x: np.hstack([X_num[x], X_cat[x]]) for x in X_num}\n",
        "            )\n",
        "            X_cat = None\n",
        "\n",
        "    y, y_info = build_target(dataset.y, transformations.y_policy, dataset.task_type)\n",
        "\n",
        "    dataset = replace(dataset, X_num=X_num, X_cat=X_cat, y=y, y_info=y_info)\n",
        "    dataset.num_transform = num_transform\n",
        "    dataset.cat_transform = cat_transform\n",
        "\n",
        "    if cache_path is not None:\n",
        "        util.dump_pickle((transformations, dataset), cache_path)\n",
        "    # if return_transforms:\n",
        "        # return dataset, num_transform, cat_transform\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def build_dataset(\n",
        "    path: Union[str, Path],\n",
        "    transformations: Transformations,\n",
        "    cache: bool\n",
        ") -> Dataset:\n",
        "    path = Path(path)\n",
        "    dataset = Dataset.from_dir(path)\n",
        "    return transform_dataset(dataset, transformations, path if cache else None)\n",
        "\n",
        "\n",
        "def prepare_tensors(\n",
        "    dataset: Dataset, device: Union[str, torch.device]\n",
        ") -> Tuple[Optional[TensorDict], Optional[TensorDict], TensorDict]:\n",
        "    X_num, X_cat, Y = (\n",
        "        None if x is None else {k: torch.as_tensor(v) for k, v in x.items()}\n",
        "        for x in [dataset.X_num, dataset.X_cat, dataset.y]\n",
        "    )\n",
        "    if device.type != 'cpu':\n",
        "        X_num, X_cat, Y = (\n",
        "            None if x is None else {k: v.to(device) for k, v in x.items()}\n",
        "            for x in [X_num, X_cat, Y]\n",
        "        )\n",
        "    assert X_num is not None\n",
        "    assert Y is not None\n",
        "    if not dataset.is_multiclass:\n",
        "        Y = {k: v.float() for k, v in Y.items()}\n",
        "    return X_num, X_cat, Y\n",
        "\n",
        "###############\n",
        "## DataLoader##\n",
        "###############\n",
        "\n",
        "class TabDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self, dataset : Dataset, split : Literal['train', 'val', 'test']\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.X_num = torch.from_numpy(dataset.X_num[split]) if dataset.X_num is not None else None\n",
        "        self.X_cat = torch.from_numpy(dataset.X_cat[split]) if dataset.X_cat is not None else None\n",
        "        self.y = torch.from_numpy(dataset.y[split])\n",
        "\n",
        "        assert self.y is not None\n",
        "        assert self.X_num is not None or self.X_cat is not None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        out_dict = {\n",
        "            'y': self.y[idx].long() if self.y is not None else None,\n",
        "        }\n",
        "\n",
        "        x = np.empty((0,))\n",
        "        if self.X_num is not None:\n",
        "            x = self.X_num[idx]\n",
        "        if self.X_cat is not None:\n",
        "            x = torch.cat([x, self.X_cat[idx]], dim=0)\n",
        "        return x.float(), out_dict\n",
        "\n",
        "def prepare_dataloader(\n",
        "    dataset : Dataset,\n",
        "    split : str,\n",
        "    batch_size: int,\n",
        "):\n",
        "\n",
        "    torch_dataset = TabDataset(dataset, split)\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        torch_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(split == 'train'),\n",
        "        num_workers=1,\n",
        "    )\n",
        "    while True:\n",
        "        yield from loader\n",
        "\n",
        "def prepare_torch_dataloader(\n",
        "    dataset : Dataset,\n",
        "    split : str,\n",
        "    shuffle : bool,\n",
        "    batch_size: int,\n",
        ") -> torch.utils.data.DataLoader:\n",
        "\n",
        "    torch_dataset = TabDataset(dataset, split)\n",
        "    loader = torch.utils.data.DataLoader(torch_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=1)\n",
        "\n",
        "    return loader\n",
        "\n",
        "def dataset_from_csv(paths : Dict[str, str], cat_features, target, T):\n",
        "    assert 'train' in paths\n",
        "    y = {}\n",
        "    X_num = {}\n",
        "    X_cat = {} if len(cat_features) else None\n",
        "    for split in paths.keys():\n",
        "        df = pd.read_csv(paths[split])\n",
        "        y[split] = df[target].to_numpy().astype(float)\n",
        "        if X_cat is not None:\n",
        "            X_cat[split] = df[cat_features].to_numpy().astype(str)\n",
        "        X_num[split] = df.drop(cat_features + [target], axis=1).to_numpy().astype(float)\n",
        "\n",
        "    dataset = Dataset(X_num, X_cat, y, {}, None, len(np.unique(y['train'])))\n",
        "    return transform_dataset(dataset, T, None)\n",
        "\n",
        "class FastTensorDataLoader:\n",
        "    \"\"\"\n",
        "    A DataLoader-like object for a set of tensors that can be much faster than\n",
        "    TensorDataset + DataLoader because dataloader grabs individual indices of\n",
        "    the dataset and calls cat (slow).\n",
        "    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n",
        "    \"\"\"\n",
        "    def __init__(self, *tensors, batch_size=32, shuffle=False):\n",
        "        \"\"\"\n",
        "        Initialize a FastTensorDataLoader.\n",
        "        :param *tensors: tensors to store. Must have the same length @ dim 0.\n",
        "        :param batch_size: batch size to load.\n",
        "        :param shuffle: if True, shuffle the data *in-place* whenever an\n",
        "            iterator is created out of this object.\n",
        "        :returns: A FastTensorDataLoader.\n",
        "        \"\"\"\n",
        "        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n",
        "        self.tensors = tensors\n",
        "\n",
        "        self.dataset_len = self.tensors[0].shape[0]\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Calculate # batches\n",
        "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
        "        if remainder > 0:\n",
        "            n_batches += 1\n",
        "        self.n_batches = n_batches\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            r = torch.randperm(self.dataset_len)\n",
        "            self.tensors = [t[r] for t in self.tensors]\n",
        "        self.i = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.i >= self.dataset_len:\n",
        "            raise StopIteration\n",
        "        batch = tuple(t[self.i:self.i+self.batch_size] for t in self.tensors)\n",
        "        self.i += self.batch_size\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_batches\n",
        "\n",
        "def prepare_fast_dataloader(\n",
        "    D : Dataset,\n",
        "    split : str,\n",
        "    batch_size: int\n",
        "):\n",
        "    if D.X_cat is not None:\n",
        "        if D.X_num is not None:\n",
        "            X = torch.from_numpy(np.concatenate([D.X_num[split], D.X_cat[split]], axis=1)).float()\n",
        "        else:\n",
        "            X = torch.from_numpy(D.X_cat[split]).float()\n",
        "    else:\n",
        "        X = torch.from_numpy(D.X_num[split]).float()\n",
        "    y = torch.from_numpy(D.y[split])\n",
        "    dataloader = FastTensorDataLoader(X, y, batch_size=batch_size, shuffle=(split=='train'))\n",
        "    while True:\n",
        "        yield from dataloader\n",
        "\n",
        "def prepare_fast_torch_dataloader(\n",
        "    D : Dataset,\n",
        "    split : str,\n",
        "    batch_size: int\n",
        "):\n",
        "    if D.X_cat is not None:\n",
        "        X = torch.from_numpy(np.concatenate([D.X_num[split], D.X_cat[split]], axis=1)).float()\n",
        "    else:\n",
        "        X = torch.from_numpy(D.X_num[split]).float()\n",
        "    y = torch.from_numpy(D.y[split])\n",
        "    dataloader = FastTensorDataLoader(X, y, batch_size=batch_size, shuffle=(split=='train'))\n",
        "    return dataloader\n",
        "\n",
        "def round_columns(X_real, X_synth, columns):\n",
        "    for col in columns:\n",
        "        uniq = np.unique(X_real[:,col])\n",
        "        dist = cdist(X_synth[:, col][:, np.newaxis].astype(float), uniq[:, np.newaxis].astype(float))\n",
        "        X_synth[:, col] = uniq[dist.argmin(axis=1)]\n",
        "    return X_synth\n",
        "\n",
        "def concat_features(D : Dataset):\n",
        "    if D.X_num is None:\n",
        "        assert D.X_cat is not None\n",
        "        X = {k: pd.DataFrame(v, columns=range(D.n_features)) for k, v in D.X_cat.items()}\n",
        "    elif D.X_cat is None:\n",
        "        assert D.X_num is not None\n",
        "        X = {k: pd.DataFrame(v, columns=range(D.n_features)) for k, v in D.X_num.items()}\n",
        "    else:\n",
        "        X = {\n",
        "            part: pd.concat(\n",
        "                [\n",
        "                    pd.DataFrame(D.X_num[part], columns=range(D.n_num_features)),\n",
        "                    pd.DataFrame(\n",
        "                        D.X_cat[part],\n",
        "                        columns=range(D.n_num_features, D.n_features),\n",
        "                    ),\n",
        "                ],\n",
        "                axis=1,\n",
        "            )\n",
        "            for part in D.y.keys()\n",
        "        }\n",
        "\n",
        "    return X\n",
        "\n",
        "def concat_to_pd(X_num, X_cat, y):\n",
        "    if X_num is None:\n",
        "        return pd.concat([\n",
        "            pd.DataFrame(X_cat, columns=list(range(X_cat.shape[1]))),\n",
        "            pd.DataFrame(y, columns=['y'])\n",
        "        ], axis=1)\n",
        "    if X_cat is not None:\n",
        "        return pd.concat([\n",
        "            pd.DataFrame(X_num, columns=list(range(X_num.shape[1]))),\n",
        "            pd.DataFrame(X_cat, columns=list(range(X_num.shape[1], X_num.shape[1] + X_cat.shape[1]))),\n",
        "            pd.DataFrame(y, columns=['y'])\n",
        "        ], axis=1)\n",
        "    return pd.concat([\n",
        "            pd.DataFrame(X_num, columns=list(range(X_num.shape[1]))),\n",
        "            pd.DataFrame(y, columns=['y'])\n",
        "        ], axis=1)\n",
        "\n",
        "def read_pure_data(path, split='train'):\n",
        "    y = np.load(os.path.join(path, f'y_{split}.npy'), allow_pickle=True)\n",
        "    X_num = None\n",
        "    X_cat = None\n",
        "    if os.path.exists(os.path.join(path, f'X_num_{split}.npy')):\n",
        "        X_num = np.load(os.path.join(path, f'X_num_{split}.npy'), allow_pickle=True)\n",
        "    if os.path.exists(os.path.join(path, f'X_cat_{split}.npy')):\n",
        "        X_cat = np.load(os.path.join(path, f'X_cat_{split}.npy'), allow_pickle=True)\n",
        "\n",
        "    return X_num, X_cat, y\n",
        "\n",
        "def read_changed_val(path, val_size=0.2):\n",
        "    path = Path(path)\n",
        "    X_num_train, X_cat_train, y_train = read_pure_data(path, 'train')\n",
        "    X_num_val, X_cat_val, y_val = read_pure_data(path, 'val')\n",
        "    is_regression = load_json(path / 'info.json')['task_type'] == 'regression'\n",
        "\n",
        "    y = np.concatenate([y_train, y_val], axis=0)\n",
        "\n",
        "    ixs = np.arange(y.shape[0])\n",
        "    if is_regression:\n",
        "        train_ixs, val_ixs = train_test_split(ixs, test_size=val_size, random_state=777)\n",
        "    else:\n",
        "        train_ixs, val_ixs = train_test_split(ixs, test_size=val_size, random_state=777, stratify=y)\n",
        "    y_train = y[train_ixs]\n",
        "    y_val = y[val_ixs]\n",
        "\n",
        "    if X_num_train is not None:\n",
        "        X_num = np.concatenate([X_num_train, X_num_val], axis=0)\n",
        "        X_num_train = X_num[train_ixs]\n",
        "        X_num_val = X_num[val_ixs]\n",
        "\n",
        "    if X_cat_train is not None:\n",
        "        X_cat = np.concatenate([X_cat_train, X_cat_val], axis=0)\n",
        "        X_cat_train = X_cat[train_ixs]\n",
        "        X_cat_val = X_cat[val_ixs]\n",
        "\n",
        "    return X_num_train, X_cat_train, y_train, X_num_val, X_cat_val, y_val\n",
        "\n",
        "#############\n",
        "\n",
        "def load_dataset_info(dataset_dir_name: str) -> Dict[str, Any]:\n",
        "    path = Path(\"data/\" + dataset_dir_name)\n",
        "    info = util.load_json(path / 'info.json')\n",
        "    info['size'] = info['train_size'] + info['val_size'] + info['test_size']\n",
        "    info['n_features'] = info['n_num_features'] + info['n_cat_features']\n",
        "    info['path'] = path\n",
        "    return info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcbS4sYsrOrx",
        "outputId": "b9733533-3541-4d92-96a2-9f0bb6b6120b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting lib/data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/sample.py\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import zero\n",
        "import os\n",
        "from tab_ddpm.gaussian_multinomial_diffsuion import GaussianMultinomialDiffusion\n",
        "from tab_ddpm.utils import FoundNANsError\n",
        "from utils_train import get_model, make_dataset\n",
        "from lib import round_columns\n",
        "import lib\n",
        "\n",
        "def to_good_ohe(ohe, X):\n",
        "    indices = np.cumsum([0] + ohe._n_features_outs)\n",
        "    Xres = []\n",
        "    for i in range(1, len(indices)):\n",
        "        x_ = np.max(X[:, indices[i - 1]:indices[i]], axis=1)\n",
        "        t = X[:, indices[i - 1]:indices[i]] - x_.reshape(-1, 1)\n",
        "        Xres.append(np.where(t >= 0, 1, 0))\n",
        "    return np.hstack(Xres)\n",
        "\n",
        "def sample(\n",
        "    parent_dir,\n",
        "    real_data_path = 'data/higgs-small',\n",
        "    batch_size = 2000,\n",
        "    num_samples = 0,\n",
        "    model_type = 'mlp',\n",
        "    model_params = None,\n",
        "    model_path = None,\n",
        "    num_timesteps = 1000,\n",
        "    gaussian_loss_type = 'mse',\n",
        "    scheduler = 'cosine',\n",
        "    T_dict = None,\n",
        "    num_numerical_features = 0,\n",
        "    disbalance = None,\n",
        "    device = torch.device('cuda:1'),\n",
        "    seed = 0,\n",
        "    change_val = False\n",
        "):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    T = lib.Transformations(**T_dict)\n",
        "    D = make_dataset(\n",
        "        real_data_path,\n",
        "        T,\n",
        "        num_classes=model_params['num_classes'],\n",
        "        is_y_cond=model_params['is_y_cond'],\n",
        "        change_val=change_val\n",
        "    )\n",
        "\n",
        "    K = np.array(D.get_category_sizes('train'))\n",
        "    if len(K) == 0 or T_dict['cat_encoding'] == 'one-hot':\n",
        "        K = np.array([0])\n",
        "\n",
        "    num_numerical_features_ = D.X_num['train'].shape[1] if D.X_num is not None else 0\n",
        "    d_in = np.sum(K) + num_numerical_features_\n",
        "    model_params['d_in'] = int(d_in)\n",
        "    model = get_model(\n",
        "        model_type,\n",
        "        model_params,\n",
        "        num_numerical_features_,\n",
        "        category_sizes=D.get_category_sizes('train')\n",
        "    )\n",
        "\n",
        "    model.load_state_dict(\n",
        "        torch.load(model_path, map_location=\"cpu\")\n",
        "    )\n",
        "\n",
        "    diffusion = GaussianMultinomialDiffusion(\n",
        "        K,\n",
        "        num_numerical_features=num_numerical_features_,\n",
        "        denoise_fn=model, num_timesteps=num_timesteps,\n",
        "        gaussian_loss_type=gaussian_loss_type, scheduler=scheduler, device=device\n",
        "    )\n",
        "\n",
        "    diffusion.to(device)\n",
        "    diffusion.eval()\n",
        "\n",
        "    _, empirical_class_dist = torch.unique(torch.from_numpy(D.y['train']), return_counts=True)\n",
        "    # empirical_class_dist = empirical_class_dist.float() + torch.tensor([-5000., 10000.]).float()\n",
        "    if disbalance == 'fix':\n",
        "        empirical_class_dist[0], empirical_class_dist[1] = empirical_class_dist[1], empirical_class_dist[0]\n",
        "        x_gen, y_gen = diffusion.sample_all(num_samples, batch_size, empirical_class_dist.float(), ddim=False)\n",
        "\n",
        "    elif disbalance == 'fill':\n",
        "        ix_major = empirical_class_dist.argmax().item()\n",
        "        val_major = empirical_class_dist[ix_major].item()\n",
        "        x_gen, y_gen = [], []\n",
        "        for i in range(empirical_class_dist.shape[0]):\n",
        "            if i == ix_major:\n",
        "                continue\n",
        "            distrib = torch.zeros_like(empirical_class_dist)\n",
        "            distrib[i] = 1\n",
        "            num_samples = val_major - empirical_class_dist[i].item()\n",
        "            x_temp, y_temp = diffusion.sample_all(num_samples, batch_size, distrib.float(), ddim=False)\n",
        "            x_gen.append(x_temp)\n",
        "            y_gen.append(y_temp)\n",
        "\n",
        "        x_gen = torch.cat(x_gen, dim=0)\n",
        "        y_gen = torch.cat(y_gen, dim=0)\n",
        "\n",
        "    else:\n",
        "        x_gen, y_gen = diffusion.sample_all(num_samples, batch_size, empirical_class_dist.float(), ddim=False)\n",
        "\n",
        "\n",
        "    # try:\n",
        "    # except FoundNANsError as ex:\n",
        "    #     print(\"Found NaNs during sampling!\")\n",
        "    #     loader = lib.prepare_fast_dataloader(D, 'train', 8)\n",
        "    #     x_gen = next(loader)[0]\n",
        "    #     y_gen = torch.multinomial(\n",
        "    #         empirical_class_dist.float(),\n",
        "    #         num_samples=8,\n",
        "    #         replacement=True\n",
        "    #     )\n",
        "    X_gen, y_gen = x_gen.numpy(), y_gen.numpy()\n",
        "\n",
        "    ###\n",
        "    # X_num_unnorm = X_gen[:, :num_numerical_features]\n",
        "    # lo = np.percentile(X_num_unnorm, 2.5, axis=0)\n",
        "    # hi = np.percentile(X_num_unnorm, 97.5, axis=0)\n",
        "    # idx = (lo < X_num_unnorm) & (hi > X_num_unnorm)\n",
        "    # X_gen = X_gen[np.all(idx, axis=1)]\n",
        "    # y_gen = y_gen[np.all(idx, axis=1)]\n",
        "    ###\n",
        "\n",
        "    num_numerical_features = num_numerical_features + int(D.is_regression and not model_params[\"is_y_cond\"])\n",
        "\n",
        "    X_num_ = X_gen\n",
        "    if num_numerical_features < X_gen.shape[1]:\n",
        "        np.save(os.path.join(parent_dir, 'X_cat_unnorm'), X_gen[:, num_numerical_features:])\n",
        "        # _, _, cat_encoder = lib.cat_encode({'train': X_cat_real}, T_dict['cat_encoding'], y_real, T_dict['seed'], True)\n",
        "        if T_dict['cat_encoding'] == 'one-hot':\n",
        "            X_gen[:, num_numerical_features:] = to_good_ohe(D.cat_transform.steps[0][1], X_num_[:, num_numerical_features:])\n",
        "        X_cat = D.cat_transform.inverse_transform(X_gen[:, num_numerical_features:])\n",
        "\n",
        "    if num_numerical_features_ != 0:\n",
        "        # _, normalize = lib.normalize({'train' : X_num_real}, T_dict['normalization'], T_dict['seed'], True)\n",
        "        np.save(os.path.join(parent_dir, 'X_num_unnorm'), X_gen[:, :num_numerical_features])\n",
        "        X_num_ = D.num_transform.inverse_transform(X_gen[:, :num_numerical_features])\n",
        "        X_num = X_num_[:, :num_numerical_features]\n",
        "\n",
        "        X_num_real = np.load(os.path.join(real_data_path, \"X_num_train.npy\"), allow_pickle=True)\n",
        "        disc_cols = []\n",
        "        for col in range(X_num_real.shape[1]):\n",
        "            uniq_vals = np.unique(X_num_real[:, col])\n",
        "            if len(uniq_vals) <= 32 and ((uniq_vals - np.round(uniq_vals)) == 0).all():\n",
        "                disc_cols.append(col)\n",
        "        print(\"Discrete cols:\", disc_cols)\n",
        "        if model_params['num_classes'] == 0:\n",
        "            y_gen = X_num[:, 0]\n",
        "            X_num = X_num[:, 1:]\n",
        "        if len(disc_cols):\n",
        "            X_num = round_columns(X_num_real, X_num, disc_cols)\n",
        "\n",
        "    if num_numerical_features != 0:\n",
        "        print(\"Num shape: \", X_num.shape)\n",
        "        np.save(os.path.join(parent_dir, 'X_num_train'), X_num)\n",
        "    if num_numerical_features < X_gen.shape[1]:\n",
        "        np.save(os.path.join(parent_dir, 'X_cat_train'), X_cat)\n",
        "    np.save(os.path.join(parent_dir, 'y_train'), y_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMje8wM71DGN",
        "outputId": "dcf3324c-ce46-4ff7-9b7b-bf4bf7134672"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/sample.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !conda activate tddpm\n",
        "# !cd $PROJECT_DIR\n",
        "# !wget \"https://www.dropbox.com/s/rpckvcs3vx7j605/data.tar?dl=0\" -O data.tar\n",
        "# !tar -xvf data.tar"
      ],
      "metadata": {
        "id": "KCXcRPwLJpQ8",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"data/cp\", exist_ok=True)\n",
        "os.makedirs(\"exp/cp\", exist_ok=True)\n",
        "\n",
        "df = pd.read_csv('city_payments_sector_label_fy2017.csv')\n",
        "df.to_csv('data/cp/cp.csv', index=False)"
      ],
      "metadata": {
        "id": "3G3aXeYYFn2Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Session reset, re-import required libraries and re-run the operation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Re-load the uploaded CSV file\n",
        "df = pd.read_csv(\"data/cp/cp.csv\")\n",
        "\n",
        "# Drop ID column\n",
        "df = df.drop(columns=[\"document_no\", \"check_date\", \"contract_number\", \"contract_description\", \"vendor_name\"])\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "XSvD6ir4ouWp",
        "outputId": "7e9c8838-33c4-4672-c8bc-7dc1c76431c7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0    fy  fm  dept           department_title  char_  \\\n",
              "0           0  2017  10    42                42 COMMERCE      2   \n",
              "1           1  2017  12    26  26 LICENSES & INSPECTIONS      2   \n",
              "2           2  2017   5    44                     44 LAW      2   \n",
              "3           3  2017   1    11                  11 POLICE      2   \n",
              "4           4  2017   1    23                 23 PRISONS      3   \n",
              "\n",
              "             character_title sub_obj                        sub_obj_title  \\\n",
              "0    02 PURCHASE OF SERVICES     231                  OVERTIME MEALS 0231   \n",
              "1    02 PURCHASE OF SERVICES     211                  TRANSPORTATION 0211   \n",
              "2    02 PURCHASE OF SERVICES     258                 COURT REPORTERS 0258   \n",
              "3    02 PURCHASE OF SERVICES     260  REPAIR AND MAINTENANCE CHARGES 0260   \n",
              "4  03 MATERIALS AND SUPPLIES     313                            FOOD 0313   \n",
              "\n",
              "  doc_ref_no_prefix doc_ref_no_prefix_definition  transaction_amount  \\\n",
              "0              PVXX              payment voucher               66.82   \n",
              "1              PVXX              payment voucher              127.33   \n",
              "2              PVXX              payment voucher              454.20   \n",
              "3              VCXX                  procurement               50.00   \n",
              "4              PCXX                   petty cash               71.92   \n",
              "\n",
              "                                              sector  \n",
              "0                    Accommodation and Food Services  \n",
              "1                                            Unknown  \n",
              "2   Professional, Scientific, and Technical Services  \n",
              "3                                        Information  \n",
              "4  Public Administration (government agencies and...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f51fa04-3fd2-4c94-b9e9-0daa2b0ded34\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>fy</th>\n",
              "      <th>fm</th>\n",
              "      <th>dept</th>\n",
              "      <th>department_title</th>\n",
              "      <th>char_</th>\n",
              "      <th>character_title</th>\n",
              "      <th>sub_obj</th>\n",
              "      <th>sub_obj_title</th>\n",
              "      <th>doc_ref_no_prefix</th>\n",
              "      <th>doc_ref_no_prefix_definition</th>\n",
              "      <th>transaction_amount</th>\n",
              "      <th>sector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2017</td>\n",
              "      <td>10</td>\n",
              "      <td>42</td>\n",
              "      <td>42 COMMERCE</td>\n",
              "      <td>2</td>\n",
              "      <td>02 PURCHASE OF SERVICES</td>\n",
              "      <td>231</td>\n",
              "      <td>OVERTIME MEALS 0231</td>\n",
              "      <td>PVXX</td>\n",
              "      <td>payment voucher</td>\n",
              "      <td>66.82</td>\n",
              "      <td>Accommodation and Food Services</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>12</td>\n",
              "      <td>26</td>\n",
              "      <td>26 LICENSES &amp; INSPECTIONS</td>\n",
              "      <td>2</td>\n",
              "      <td>02 PURCHASE OF SERVICES</td>\n",
              "      <td>211</td>\n",
              "      <td>TRANSPORTATION 0211</td>\n",
              "      <td>PVXX</td>\n",
              "      <td>payment voucher</td>\n",
              "      <td>127.33</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2017</td>\n",
              "      <td>5</td>\n",
              "      <td>44</td>\n",
              "      <td>44 LAW</td>\n",
              "      <td>2</td>\n",
              "      <td>02 PURCHASE OF SERVICES</td>\n",
              "      <td>258</td>\n",
              "      <td>COURT REPORTERS 0258</td>\n",
              "      <td>PVXX</td>\n",
              "      <td>payment voucher</td>\n",
              "      <td>454.20</td>\n",
              "      <td>Professional, Scientific, and Technical Services</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2017</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>11 POLICE</td>\n",
              "      <td>2</td>\n",
              "      <td>02 PURCHASE OF SERVICES</td>\n",
              "      <td>260</td>\n",
              "      <td>REPAIR AND MAINTENANCE CHARGES 0260</td>\n",
              "      <td>VCXX</td>\n",
              "      <td>procurement</td>\n",
              "      <td>50.00</td>\n",
              "      <td>Information</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2017</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>23 PRISONS</td>\n",
              "      <td>3</td>\n",
              "      <td>03 MATERIALS AND SUPPLIES</td>\n",
              "      <td>313</td>\n",
              "      <td>FOOD 0313</td>\n",
              "      <td>PCXX</td>\n",
              "      <td>petty cash</td>\n",
              "      <td>71.92</td>\n",
              "      <td>Public Administration (government agencies and...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f51fa04-3fd2-4c94-b9e9-0daa2b0ded34')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9f51fa04-3fd2-4c94-b9e9-0daa2b0ded34 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9f51fa04-3fd2-4c94-b9e9-0daa2b0ded34');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-26e1ed16-91f0-421a-8d1b-03bf3f88da6e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-26e1ed16-91f0-421a-8d1b-03bf3f88da6e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-26e1ed16-91f0-421a-8d1b-03bf3f88da6e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define numerical, categorical, and target columns\n",
        "num_cols = [\"transaction_amount\"]\n",
        "cat_cols = [\"fy\", \"sub_obj\", \"dept\", \"char_\", \"doc_ref_no_prefix\", \"sector\"]\n",
        "target_col = \"fm\"\n",
        "\n",
        "# Split into features and labels\n",
        "X_num = df[num_cols].astype(float).values\n",
        "X_cat = df[cat_cols].values\n",
        "y = df[target_col].values\n",
        "\n",
        "# First split: separate out test set (20%)\n",
        "X_num_temp, X_num_test, X_cat_temp, X_cat_test, y_temp, y_test = train_test_split(\n",
        "    X_num, X_cat, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Second split: validation set from remaining (20% of remaining = 16% overall)\n",
        "X_num_train, X_num_val, X_cat_train, X_cat_val, y_train, y_val = train_test_split(\n",
        "    X_num_temp, X_cat_temp, y_temp, test_size=0.2, random_state=0)\n",
        "\n",
        "# Create save directory\n",
        "save_dir = \"data/cp\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save 9 .npy files\n",
        "np.save(os.path.join(save_dir, \"X_num_train.npy\"), X_num_train)\n",
        "np.save(os.path.join(save_dir, \"X_num_val.npy\"), X_num_val)\n",
        "np.save(os.path.join(save_dir, \"X_num_test.npy\"), X_num_test)\n",
        "\n",
        "np.save(os.path.join(save_dir, \"X_cat_train.npy\"), X_cat_train)\n",
        "np.save(os.path.join(save_dir, \"X_cat_val.npy\"), X_cat_val)\n",
        "np.save(os.path.join(save_dir, \"X_cat_test.npy\"), X_cat_test)\n",
        "\n",
        "np.save(os.path.join(save_dir, \"y_train.npy\"), y_train)\n",
        "np.save(os.path.join(save_dir, \"y_val.npy\"), y_val)\n",
        "np.save(os.path.join(save_dir, \"y_test.npy\"), y_test)\n",
        "\n",
        "# Return confirmation summary\n",
        "{\n",
        "    \"train_size\": len(y_train),\n",
        "    \"val_size\": len(y_val),\n",
        "    \"test_size\": len(y_test),\n",
        "    \"saved_to\": save_dir\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5rIRM-WlI1L",
        "outputId": "4873c3fe-d3fe-43aa-f21d-6ee9acb1a8d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_size': 152892,\n",
              " 'val_size': 38223,\n",
              " 'test_size': 47779,\n",
              " 'saved_to': 'data/cp'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_val = le.transform(y_val)\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "import joblib\n",
        "joblib.dump(le, \"data/cp/label_encoder.pkl\")\n",
        "\n",
        "np.save(\"data/cp/y_train.npy\", y_train)\n",
        "np.save(\"data/cp/y_val.npy\", y_val)\n",
        "np.save(\"data/cp/y_test.npy\", y_test)\n",
        "\n",
        "print(np.unique(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbI_pAa-7foQ",
        "outputId": "ef713087-8648-413b-d412-4b5225c0f9d6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "info = {\n",
        "    \"name\": \"CityPayment\",\n",
        "    \"id\": \"CityPayment--id\",\n",
        "    \"task_type\": \"multiclass\",\n",
        "    \"num_classes\": 12,\n",
        "    \"n_num_features\": 1,\n",
        "    \"n_cat_features\": 6,\n",
        "    \"train_size\": 152892,\n",
        "    \"val_size\": 38223,\n",
        "    \"test_size\": 47779\n",
        "}\n",
        "\n",
        "save_path = \"data/cp/info.json\"\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "with open(save_path, \"w\") as f:\n",
        "    json.dump(info, f, indent=4)\n",
        "\n",
        "save_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4IbPrP47rkdl",
        "outputId": "1452939b-7057-4225-8129-674f0713adb8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/cp/info.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"exp/cp\", exist_ok=True)\n",
        "\n",
        "config_text = \"\"\"\n",
        "parent_dir = \"exp/cp/check\"\n",
        "real_data_path = \"data/cp/\"\n",
        "num_numerical_features = 1\n",
        "model_type = \"mlp\"\n",
        "seed = 0\n",
        "device = \"cuda:0\"\n",
        "\n",
        "[model_params]\n",
        "num_classes = 12\n",
        "is_y_cond = true\n",
        "\n",
        "[model_params.rtdl_params]\n",
        "d_layers = [\n",
        "    256,\n",
        "    256,\n",
        "]\n",
        "dropout = 0.0\n",
        "\n",
        "[diffusion_params]\n",
        "num_timesteps = 1000\n",
        "gaussian_loss_type = \"mse\"\n",
        "scheduler = \"cosine\"\n",
        "\n",
        "[train.main]\n",
        "steps = 20000\n",
        "lr = 0.0005\n",
        "weight_decay = 1e-05\n",
        "batch_size = 4096\n",
        "\n",
        "[train.T]\n",
        "seed = 0\n",
        "normalization = \"minmax\"\n",
        "num_nan_policy = \"__none__\"\n",
        "cat_nan_policy = \"__none__\"\n",
        "cat_min_frequency = \"__none__\"\n",
        "cat_encoding = \"__none__\"\n",
        "y_policy = \"default\"\n",
        "\n",
        "[sample]\n",
        "num_samples = 238894\n",
        "batch_size = 10000\n",
        "seed = 0\n",
        "\n",
        "[eval.type]\n",
        "eval_model = \"catboost\"\n",
        "eval_type = \"synthetic\"\n",
        "\n",
        "[eval.T]\n",
        "seed = 0\n",
        "normalization = \"__none__\"\n",
        "num_nan_policy = \"__none__\"\n",
        "cat_nan_policy = \"__none__\"\n",
        "cat_min_frequency = \"__none__\"\n",
        "cat_encoding = \"__none__\"\n",
        "y_policy = \"default\"\n",
        "\"\"\"\n",
        "\n",
        "with open(\"exp/cp/config.toml\", \"w\") as f:\n",
        "    f.write(config_text)"
      ],
      "metadata": {
        "id": "L91GvjfKF7Kl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/pipeline.py --config exp/cp/config.toml --train --sample"
      ],
      "metadata": {
        "id": "2yFs5XjZYz6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "052aff48-2ed2-4eb6-b2f1-29d185679d3d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[DEBUG] Inspecting dataset.X_num types before np.isnan:\n",
            "  - X_num['train']: type = <class 'numpy.ndarray'>, dtype = float64, shape = (152892, 1)\n",
            "    first row: [10967.42]\n",
            "  - X_num['val']: type = <class 'numpy.ndarray'>, dtype = float64, shape = (38223, 1)\n",
            "    first row: [180.]\n",
            "  - X_num['test']: type = <class 'numpy.ndarray'>, dtype = float64, shape = (47779, 1)\n",
            "    first row: [249.24]\n",
            "[  1 209  57   6  12  25]\n",
            "311\n",
            "{'num_classes': 12, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 311}\n",
            "mlp\n",
            "Step 500/20000 MLoss: 1.4043 GLoss: 0.8907 Sum: 2.295\n",
            "Step 1000/20000 MLoss: 1.2733 GLoss: 0.7613 Sum: 2.0346\n",
            "Step 1500/20000 MLoss: 1.2688 GLoss: 0.6515 Sum: 1.9203\n",
            "Step 2000/20000 MLoss: 1.2212 GLoss: 0.3916 Sum: 1.6128\n",
            "Step 2500/20000 MLoss: 1.1777 GLoss: 0.259 Sum: 1.4367\n",
            "Step 3000/20000 MLoss: 1.1448 GLoss: 0.2311 Sum: 1.3759000000000001\n",
            "Step 3500/20000 MLoss: 1.1296 GLoss: 0.2539 Sum: 1.3835\n",
            "Step 4000/20000 MLoss: 1.1159 GLoss: 0.2112 Sum: 1.3271\n",
            "Step 4500/20000 MLoss: 1.1027 GLoss: 0.16 Sum: 1.2627\n",
            "Step 5000/20000 MLoss: 1.0887 GLoss: 0.1722 Sum: 1.2609\n",
            "Step 5500/20000 MLoss: 1.0847 GLoss: 0.154 Sum: 1.2387\n",
            "Step 6000/20000 MLoss: 1.0613 GLoss: 0.1234 Sum: 1.1846999999999999\n",
            "Step 6500/20000 MLoss: 1.0626 GLoss: 0.1171 Sum: 1.1797\n",
            "Step 7000/20000 MLoss: 1.0554 GLoss: 0.1294 Sum: 1.1847999999999999\n",
            "Step 7500/20000 MLoss: 1.0517 GLoss: 0.079 Sum: 1.1307\n",
            "Step 8000/20000 MLoss: 1.0374 GLoss: 0.0856 Sum: 1.123\n",
            "Step 8500/20000 MLoss: 1.0459 GLoss: 0.0905 Sum: 1.1364\n",
            "Step 9000/20000 MLoss: 1.0324 GLoss: 0.0832 Sum: 1.1156\n",
            "Step 9500/20000 MLoss: 1.0233 GLoss: 0.0646 Sum: 1.0879\n",
            "Step 10000/20000 MLoss: 1.0266 GLoss: 0.072 Sum: 1.0986\n",
            "Step 10500/20000 MLoss: 1.0271 GLoss: 0.0516 Sum: 1.0787\n",
            "Step 11000/20000 MLoss: 1.0214 GLoss: 0.0767 Sum: 1.0981\n",
            "Step 11500/20000 MLoss: 1.0125 GLoss: 0.0504 Sum: 1.0629\n",
            "Step 12000/20000 MLoss: 1.0105 GLoss: 0.0445 Sum: 1.055\n",
            "Step 12500/20000 MLoss: 1.0117 GLoss: 0.0392 Sum: 1.0509\n",
            "Step 13000/20000 MLoss: 1.0094 GLoss: 0.0406 Sum: 1.05\n",
            "Step 13500/20000 MLoss: 1.0019 GLoss: 0.0349 Sum: 1.0368\n",
            "Step 14000/20000 MLoss: 1.0025 GLoss: 0.0293 Sum: 1.0318\n",
            "Step 14500/20000 MLoss: 1.0025 GLoss: 0.0275 Sum: 1.03\n",
            "Step 15000/20000 MLoss: 1.0019 GLoss: 0.0237 Sum: 1.0256\n",
            "Step 15500/20000 MLoss: 0.9916 GLoss: 0.0227 Sum: 1.0143\n",
            "Step 16000/20000 MLoss: 0.9949 GLoss: 0.016 Sum: 1.0109\n",
            "Step 16500/20000 MLoss: 0.9981 GLoss: 0.0195 Sum: 1.0176\n",
            "Step 17000/20000 MLoss: 0.9919 GLoss: 0.0145 Sum: 1.0064\n",
            "Step 17500/20000 MLoss: 0.9909 GLoss: 0.0143 Sum: 1.0052\n",
            "Step 18000/20000 MLoss: 0.9902 GLoss: 0.012 Sum: 1.0022\n",
            "Step 18500/20000 MLoss: 0.9973 GLoss: 0.0112 Sum: 1.0085\n",
            "Step 19000/20000 MLoss: 0.9929 GLoss: 0.0105 Sum: 1.0034\n",
            "Step 19500/20000 MLoss: 0.9887 GLoss: 0.0102 Sum: 0.9989\n",
            "Step 20000/20000 MLoss: 0.9841 GLoss: 0.0099 Sum: 0.994\n",
            "\n",
            "[DEBUG] Inspecting dataset.X_num types before np.isnan:\n",
            "  - X_num['train']: type = <class 'numpy.ndarray'>, dtype = float64, shape = (152892, 1)\n",
            "    first row: [10967.42]\n",
            "  - X_num['val']: type = <class 'numpy.ndarray'>, dtype = float64, shape = (38223, 1)\n",
            "    first row: [180.]\n",
            "  - X_num['test']: type = <class 'numpy.ndarray'>, dtype = float64, shape = (47779, 1)\n",
            "    first row: [249.24]\n",
            "mlp\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Sample timestep    0\n",
            "Discrete cols: []\n",
            "Num shape:  (238894, 1)\n",
            "Elapsed time: 0.00 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "parent_dir = \"exp/cp/check\"\n",
        "\n",
        "X_num = np.load(os.path.join(parent_dir, \"X_num_train.npy\"))\n",
        "X_cat = np.load(os.path.join(parent_dir, \"X_cat_train.npy\"), allow_pickle=True)\n",
        "y = np.load(os.path.join(parent_dir, \"y_train.npy\"))\n",
        "\n",
        "# num_cols = [\"LIMIT_BAL\", \"AGE\",\n",
        "#             \"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\",\n",
        "#             \"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\"]\n",
        "\n",
        "# cat_cols = [\"SEX\", \"EDUCATION\", \"MARRIAGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\",\n",
        "#             \"PAY_4\", \"PAY_5\", \"PAY_6\"]\n",
        "\n",
        "# target_col = \"default payment next month\"\n",
        "\n",
        "df_num = pd.DataFrame(X_num, columns=num_cols)\n",
        "df_cat = pd.DataFrame(X_cat, columns=cat_cols)\n",
        "df_y = pd.Series(y, name=target_col)\n",
        "df_y += 1\n",
        "\n",
        "df = pd.concat([df_num, df_cat, df_y], axis=1)\n",
        "\n",
        "output_path = os.path.join(parent_dir, \"generated_cp.csv\")\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"CSV has been saved to: {output_path}\")"
      ],
      "metadata": {
        "id": "IPAbD4Ph4sra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1d2dfc-ca61-481a-ca9c-5788e8a6fb1f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV has been saved to: exp/cp/check/generated_cp.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('exp/cp/check/generated_cp.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "htm0wR8syKRK",
        "outputId": "e7b9d8af-77db-42aa-c5a8-77ddfb19c4d5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_73ac8d39-f11f-4c93-8187-a1ad33ca1c4b\", \"generated_cp.csv\", 16820455)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sdv.evaluation.single_table import evaluate_quality\n",
        "from sdv.metadata import SingleTableMetadata\n",
        "\n",
        "real_data = pd.read_csv(\"data/cp/cp.csv\", index_col=0)\n",
        "real_data = real_data.drop(columns=[\"document_no\", \"check_date\", \"contract_number\", \"contract_description\",  \"department_title\", \"character_title\", \"sub_obj_title\", \"doc_ref_no_prefix_definition\", \"vendor_name\"])\n",
        "synthetic_data = pd.read_csv(\"exp/cp/check/generated_cp.csv\")\n",
        "\n",
        "target_column = \"fm\"\n",
        "\n",
        "metadata = SingleTableMetadata()\n",
        "metadata.detect_from_dataframe(data=real_data)\n",
        "\n",
        "quality_report = evaluate_quality(\n",
        "    real_data=real_data,\n",
        "    synthetic_data=synthetic_data,\n",
        "    metadata=metadata,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "jCHXoAubCax9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70e75a4-e476-4461-985e-c5c96ac46b49"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating report ...\n",
            "\n",
            "\n",
            "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "(1/2) Evaluating Column Shapes: :   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "(1/2) Evaluating Column Shapes: :  38%|███▊      | 3/8 [00:00<00:00, 10.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "(1/2) Evaluating Column Shapes: :  62%|██████▎   | 5/8 [00:00<00:00,  8.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "(1/2) Evaluating Column Shapes: : 100%|██████████| 8/8 [00:00<00:00,  9.93it/s]\n",
            "\n",
            "\n",
            "  0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :   0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  14%|█▍        | 4/28 [00:00<00:01, 13.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  21%|██▏       | 6/28 [00:00<00:01, 11.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  29%|██▊       | 8/28 [00:00<00:01, 10.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  36%|███▌      | 10/28 [00:00<00:01,  9.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  39%|███▉      | 11/28 [00:01<00:02,  8.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  46%|████▋     | 13/28 [00:01<00:01,  8.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  54%|█████▎    | 15/28 [00:01<00:01,  8.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  57%|█████▋    | 16/28 [00:01<00:01,  7.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  64%|██████▍   | 18/28 [00:02<00:01,  7.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  68%|██████▊   | 19/28 [00:02<00:01,  6.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  71%|███████▏  | 20/28 [00:02<00:01,  6.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  79%|███████▊  | 22/28 [00:02<00:00,  7.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  82%|████████▏ | 23/28 [00:02<00:00,  7.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  86%|████████▌ | 24/28 [00:03<00:00,  6.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  89%|████████▉ | 25/28 [00:03<00:00,  6.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  93%|█████████▎| 26/28 [00:03<00:00,  6.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: :  96%|█████████▋| 27/28 [00:03<00:00,  6.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "(2/2) Evaluating Column Pair Trends: : 100%|██████████| 28/28 [00:03<00:00,  7.53it/s]\n",
            "\n",
            "Overall Quality Score: 89.15%\n",
            "\n",
            "Properties:\n",
            "- Column Shapes: 92.98%\n",
            "- Column Pair Trends: 85.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot Column Shapes -> referred to the \"Fidelity Column\" in the paper\n",
        "fig = quality_report.get_visualization(property_name='Column Shapes')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "vbTrn5ssDd_o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "outputId": "369727ca-5b06-4886-b0bf-42b8cab400c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r(2/2) Evaluating Column Pair Trends: :   0%|          | 0/45 [22:51<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"6f1fb523-f197-4652-af00-5f716208e461\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6f1fb523-f197-4652-af00-5f716208e461\")) {                    Plotly.newPlot(                        \"6f1fb523-f197-4652-af00-5f716208e461\",                        [{\"alignmentgroup\":\"True\",\"customdata\":[[\"KSComplement\"],[\"KSComplement\"],[\"KSComplement\"],[\"KSComplement\"],[\"KSComplement\"]],\"hovertemplate\":\"\\u003cb\\u003e%{hovertext}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eMetric=%{customdata[0]}\\u003cbr\\u003eScore=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"hovertext\":[\"fy\",\"fm\",\"dept\",\"char_\",\"transaction_amount\"],\"legendgroup\":\"KSComplement\",\"marker\":{\"color\":\"#000036\",\"pattern\":{\"shape\":\"\"}},\"name\":\"KSComplement\",\"offsetgroup\":\"KSComplement\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"fy\",\"fm\",\"dept\",\"char_\",\"transaction_amount\"],\"xaxis\":\"x\",\"y\":[1.0,0.9976977236766098,0.9939680360327174,0.990163001163696,0.5091839895518515],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"customdata\":[[\"TVComplement\"],[\"TVComplement\"],[\"TVComplement\"]],\"hovertemplate\":\"\\u003cb\\u003e%{hovertext}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eMetric=%{customdata[0]}\\u003cbr\\u003eScore=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"hovertext\":[\"sub_obj\",\"doc_ref_no_prefix\",\"sector\"],\"legendgroup\":\"TVComplement\",\"marker\":{\"color\":\"#03AFF1\",\"pattern\":{\"shape\":\"\\u002f\"}},\"name\":\"TVComplement\",\"offsetgroup\":\"TVComplement\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"sub_obj\",\"doc_ref_no_prefix\",\"sector\"],\"xaxis\":\"x\",\"y\":[0.9733187103903823,0.9902445278057984,0.9839133674349293],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Column\"},\"categoryorder\":\"total ascending\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Score\"},\"range\":[0,1]},\"legend\":{\"title\":{\"text\":\"Metric\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Data Quality: Column Shapes (Average Score=0.93)\"},\"barmode\":\"relative\",\"margin\":{\"t\":150},\"font\":{\"size\":18},\"plot_bgcolor\":\"#F5F5F8\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6f1fb523-f197-4652-af00-5f716208e461');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot Column Pair Trends -> referred to the \"Fidelity Row\" in the paper\n",
        "fig = quality_report.get_visualization(property_name='Column Pair Trends')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "X_HWk9s-DmYQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "a05679b0-9d03-41a2-ddfc-614cdbbc24ef"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1bbc7c93-edbd-4432-b966-9038cded627c\" class=\"plotly-graph-div\" style=\"height:900px; width:900px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1bbc7c93-edbd-4432-b966-9038cded627c\")) {                    Plotly.newPlot(                        \"1bbc7c93-edbd-4432-b966-9038cded627c\",                        [{\"coloraxis\":\"coloraxis\",\"hovertemplate\":\"\\u003cb\\u003eColumn Pair\\u003c\\u002fb\\u003e\\u003cbr\\u003e(%{x},%{y})\\u003cbr\\u003e\\u003cbr\\u003eSimilarity: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"x\":[\"fy\",\"fm\",\"dept\",\"char_\",\"sub_obj\",\"doc_ref_no_prefix\",\"transaction_amount\",\"sector\"],\"y\":[\"fy\",\"fm\",\"dept\",\"char_\",\"sub_obj\",\"doc_ref_no_prefix\",\"transaction_amount\",\"sector\"],\"z\":[[1.0,null,null,null,0.973,0.99,null,0.984],[null,1.0,0.991,0.995,0.92,0.954,0.997,0.96],[null,0.991,1.0,0.994,0.954,0.985,0.999,0.974],[null,0.995,0.994,1.0,0.971,0.981,0.983,0.975],[0.973,0.92,0.954,0.971,1.0,0.966,0.0,0.952],[0.99,0.954,0.985,0.981,0.966,1.0,0.0,0.977],[null,0.997,0.999,0.983,0.0,0.0,1.0,0.0],[0.984,0.96,0.974,0.975,0.952,0.977,0.0,1.0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"coloraxis\":\"coloraxis2\",\"customdata\":[[1.0,0.004,0.006,0.004],[0.004,1.0,0.075,-0.0],[0.006,0.075,1.0,0.003],[0.004,-0.0,0.003,1.0]],\"hovertemplate\":\"\\u003cb\\u003eCorrelation\\u003c\\u002fb\\u003e\\u003cbr\\u003e(%{x},%{y})\\u003cbr\\u003e\\u003cbr\\u003eSynthetic: %{z}\\u003cbr\\u003e(vs. Real: %{customdata})\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"x\":[\"fm\",\"dept\",\"char_\",\"transaction_amount\"],\"y\":[\"fm\",\"dept\",\"char_\",\"transaction_amount\"],\"z\":[[1.0,0.022,0.017,-0.001],[0.022,1.0,0.064,-0.002],[0.017,0.064,1.0,0.038],[-0.001,-0.002,0.038,1.0]],\"type\":\"heatmap\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"coloraxis\":\"coloraxis2\",\"customdata\":[[1.0,0.022,0.017,-0.001],[0.022,1.0,0.064,-0.002],[0.017,0.064,1.0,0.038],[-0.001,-0.002,0.038,1.0]],\"hovertemplate\":\"\\u003cb\\u003eCorrelation\\u003c\\u002fb\\u003e\\u003cbr\\u003e(%{x},%{y})\\u003cbr\\u003e\\u003cbr\\u003eSynthetic: %{z}\\u003cbr\\u003e(vs. Real: %{customdata})\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"x\":[\"fm\",\"dept\",\"char_\",\"transaction_amount\"],\"y\":[\"fm\",\"dept\",\"char_\",\"transaction_amount\"],\"z\":[[1.0,0.004,0.006,0.004],[0.004,1.0,0.075,-0.0],[0.006,0.075,1.0,0.003],[0.004,-0.0,0.003,1.0]],\"type\":\"heatmap\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.26,0.74],\"tickangle\":45},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.625,1.0],\"autorange\":\"reversed\"},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.0,0.45],\"tickangle\":45},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,0.375],\"autorange\":\"reversed\"},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.55,1.0],\"tickangle\":45,\"matches\":\"x2\"},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.375],\"visible\":false,\"matches\":\"y2\",\"autorange\":\"reversed\"},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Real vs. Synthetic Similarity\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Numerical Correlation (Real Data)\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Numerical Correlation (Synthetic Data)\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Data Quality: Column Pair Trends (Average Score=0.85)\"},\"coloraxis\":{\"colorbar\":{\"len\":0.5,\"x\":0.8,\"y\":0.8},\"cmin\":0,\"cmax\":1,\"colorscale\":[[0.0,\"#FF0000\"],[0.5,\"#F16141\"],[1.0,\"#36B37E\"]]},\"coloraxis2\":{\"colorbar\":{\"len\":0.5,\"y\":0.2},\"cmin\":-1,\"cmax\":1,\"colorscale\":[[0.0,\"#03AFF1\"],[0.5,\"#000036\"],[1.0,\"#01E0C9\"]]},\"font\":{\"size\":18},\"height\":900,\"width\":900},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1bbc7c93-edbd-4432-b966-9038cded627c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}